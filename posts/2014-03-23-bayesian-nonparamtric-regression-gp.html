<meta name="title" content="Nonparametric Bayesian Regression with Gaussian Processes" />
<meta name="tags" content="Gaussian Processes, Bayesian Statistics, Nonparametric Statistics" />
<meta name="date" content="2014-03-23" />
<meta name="has_math" content="true" /><p><a href="http://en.wikipedia.org/wiki/Non-parametric_statistics">Nonparametric statistics</a> is a branch of statistics concerned with analyzing data without assuming that the data are generated from any particular probability distribution. Since they make fewer assumptions about the process that generated the data, nonparametric tests are often more generally applicable than their parametric counterparts. Nonparametric methods are also generally more <a href="http://en.wikipedia.org/wiki/Robust_statistics">robust</a> than their parametric counterparts. These advantages are balanced by the fact that when the data are generated by a known probability distribution, the appropriate parametric tests are more <a href="http://en.wikipedia.org/wiki/Parametric_statistics">powerful</a>.</p>
<p>In this post, we’ll explore a Bayesian approach to nonparametric regression, which allows us to model complex functions with relatively weak assumptions.</p>
<h4 id="a-prior-on-functions">A Prior on Functions</h4>
<p>We will study the model <span class="math inline">\(y = f(x) + \varepsilon\)</span>, where <span class="math inline">\(f\)</span> is the true regression function we wish to model, and <span class="math inline">\(\varepsilon\)</span> is Gaussian noise. Given observations <span class="math inline">\((x_1, y_1), \ldots, (x_n, y_n)\)</span> from this model, we wish to predict the value of <span class="math inline">\(f\)</span> at new <span class="math inline">\(x\)</span>-values. Let <span class="math inline">\(X = (x_1, \ldots, x_n)^\top\)</span> be the column vector of the observed <span class="math inline">\(x\)</span>-values, and similarly let <span class="math inline">\(Y = (y_1, \ldots, y_n)^\top\)</span>. Our goal is to use these observations to predict the value of <span class="math inline">\(f\)</span> at any given point with as much accuracy as possible.</p>
<p>The Bayesian approach to this problem is to calculate the <a href="http://en.wikipedia.org/wiki/Posterior_predictive_distribution">posterior predictive distribution</a>, <span class="math inline">\(p(f_* | X, Y, x_*)\)</span> of the value of <span class="math inline">\(f\)</span> at a point <span class="math inline">\(x_*\)</span>. With this distribution in hand, we, if necessary, employ <a href="http://en.wikipedia.org/wiki/Decision_theory">decision-theoretic</a> machinery to produce point estimates for the value of <span class="math inline">\(f\)</span> at <span class="math inline">\(x_*\)</span>.</p>
<p>As the nonparametric approach makes as few assumptions about the regression function as possible, a natural first step would seem to be to place a prior distribution <span class="math inline">\(\pi(f)\)</span> on all possible regression functions. In this case, the posterior predictive distribution is</p>
<p><span class="math display">\[\begin{align*}
p(f_* | X, Y, x_*)
    &amp; = \int_f p(f_* | f, x_*)\ p(f | X, Y)\ df \\
    &amp; \propto \int_f p(f_* | f, x_*)\ p(Y | f, X)\ \pi(f) df.
\end{align*}\]</span></p>
<p>Observant readers have certainly noticed that above we integrate with respect to an unspecified measure <span class="math inline">\(df\)</span>. If we make the often-reasonable assumption that <span class="math inline">\(f\)</span> is continuous on a compact interval <span class="math inline">\([0, T]\)</span> (for <span class="math inline">\(T &lt; \infty\)</span>), we may choose <span class="math inline">\(df\)</span> to be the <a href="http://en.wikipedia.org/wiki/Classical_Wiener_space#Classical_Wiener_measure">Weiner measure</a>, which is closely related to <a href="http://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>. Unfortunately, this approach has a few drawbacks. First, it may be difficult to choose a decent prior on the contiuous functions on the interval <span class="math inline">\([0, T]\)</span> (even the uniform prior will be improper). Even if we manage to choose a good prior, the integral with respect to the Weiner measure may not be easy (or even possible) to calculate or approximate.</p>
<h4 id="gaussian-processes">Gaussian Processes</h4>
<p>The key realization about this situation that allows us to sidestep the difficulty in calculating the posterior predictive distribution in the manner detailed above is to consider the joint distribution of the observed and predicted values of <span class="math inline">\(f\)</span>. Suppose that given our observations, we want to predict the value of <span class="math inline">\(f\)</span> at the points <span class="math inline">\(x_{*, 1}, \ldots x_{*, m}\)</span>. Again, let <span class="math inline">\(X_* = (x_{*, 1}, \ldots, x_{*, m})^\top\)</span> and <span class="math inline">\(Y_* = (f(x_{*, 1}), \ldots, f(x_{*, m}))^\top\)</span>. With this notation, we want to model the joint distribution of the random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y_*\)</span>. After choosing a joint distribution, we can condition on the observed value of <span class="math inline">\(Y\)</span> (and <span class="math inline">\(X\)</span> and <span class="math inline">\(X_*\)</span>) to obtain a posterior predictive distribution for <span class="math inline">\(Y_*\)</span>. In this approach, specifying the joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y_*\)</span> has taken the place of specifying a prior distribution on regression functions.</p>
<p>We obtain a flexible model for the joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y_*\)</span> by considering the set <span class="math inline">\(\{f(x) | x \in \mathbb{R}\}\)</span> to be a <a href="http://en.wikipedia.org/wiki/Gaussian_process">Gaussian Process</a>. That is, for any finite number of points <span class="math inline">\(x_1, \ldots, x_n \in \mathbb{R}\)</span>, the random variables <span class="math inline">\(f(x_1), \ldots, f(x_n)\)</span> have a <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal distribution</a>. While at first it may seem that modelling <span class="math inline">\(f\)</span> with a Gaussian process is a parametric approach to the problem, mean vector and covarience matrix of the multivariate normal (generally) depend explicitly on the points <span class="math inline">\(x_1, \ldots, x_n\)</span>, making this approach quite flexible and nonparametric. The Gaussian process for <span class="math inline">\(f\)</span> is specified by a mean function <span class="math inline">\(m(x)\)</span> and a covariance function <span class="math inline">\(k(x, x&#39;)\)</span>. In this context, we write that <span class="math inline">\(f \sim GP(m, k)\)</span>. The advantage of this approach is that the Gaussian process model is quite flexible, due to the ability to specify different mean and covariance functions, while still allowing for exact calculation of the predictions.</p>
<p>For the purposes of this post, we will assume that the mean of our Gaussian process is zero; we will see later that this is not a terribly restrictive assumption. Much more important than the choice of a mean function is the choice of a covariance function, <span class="math inline">\(k\)</span>. In general, <span class="math inline">\(k\)</span> must be <a href="http://en.wikipedia.org/wiki/Positive-definite_matrix">positive-definite</a> in the sense that for any <span class="math inline">\(X = (x_1, \ldots x_n)\)</span>, <span class="math inline">\(k(X, X) = (k(x_i, x_j))_{i, j}\)</span> is a positive-definite matrix. This restriction is necessary in order for the covariance matrices of the multivariate normal distributions obtained from the Gaussian process to be sensible. There are <a href="http://en.wikipedia.org/wiki/Gaussian_process#Usual_covariance_functions">far too many</a> choices of covariance function to list in detail here. In this post, we will use the common squared-exponential covariance function,</p>
<p><span class="math display">\[k(x, x&#39;) = \exp\left(-\frac{(x - x&#39;)^2}{2 \ell^2}\right).\]</span></p>
<p>The parameter <span class="math inline">\(\ell\)</span> controls how quickly the function <span class="math inline">\(f\)</span> may fluctuate. Small values of <span class="math inline">\(\ell\)</span> allow <span class="math inline">\(f\)</span> to fluctuate rapidly, while large values of <span class="math inline">\(\ell\)</span> allow <span class="math inline">\(f\)</span> to fluctuate slowly. The following diagram shows samples from Gaussian processes with different values of <span class="math inline">\(\ell\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k(x1, x2, l<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>np.subtract.outer(x1, x2)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> l<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_gp(xs, m<span class="op">=</span><span class="va">None</span>, k<span class="op">=</span>k, l<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> np.zeros_like(xs)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> m(xs)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> k(xs, xs, l)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.multivariate_normal(mean, cov, size<span class="op">=</span>size)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> size <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> samples[<span class="dv">0</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> samples</span></code></pre></div>
<center>
<img src="/resources/bayesian-nonparametric-gp/samples.png" title="fig:" alt="png" />
</center>
<h4 id="gaussian-process-regression">Gaussian Process Regression</h4>
<p>Now that we have introducted the basics of Gaussian processes, it is time to use them for regression. Using the notation from above, the joint distribution of <span class="math inline">\(Y\)</span>, the values of <span class="math inline">\(f\)</span> at the observed points, and <span class="math inline">\(Y_*\)</span>, the values of <span class="math inline">\(f\)</span> at the points to be predicted, is normal with mean zero and covariance matrix</p>
<p><span class="math display">\[\Sigma = \begin{pmatrix}
k(X, X) + \sigma^2 I &amp; k(X, X_*) \\
k(X_*, X) &amp; k(X_*, X_*)
\end{pmatrix}.\]</span></p>
<p>In <span class="math inline">\(\Sigma\)</span>, only the top left block contains a noise term. The top right and bottom left blocks have no noise term because the entries of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y_*\)</span> are uncorrelated (since the noise is i.i.d). The bottom right block has no noise term because we want to predict the actual value of <span class="math inline">\(f\)</span> at each point <span class="math inline">\(x_*, i\)</span>, not its value plus noise.</p>
<p><a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">Conditioning</a> on the observations <span class="math inline">\(Y = y\)</span>, we get that <span class="math inline">\(Y_* | Y = y\)</span> is normal with mean <span class="math inline">\(\mu_y = k(X_*, X) (k(X, X) + \sigma^2 I)^{-1} y\)</span> and covariance <span class="math inline">\(\Sigma_y = k(X_*, X_*) - k(X_*, X) (k(X, X) + \sigma^2 I)^{-1} k(X, X_*)\)</span>.</p>
<h4 id="a-toy-example">A Toy Example</h4>
<p>As a toy example, we will use Gaussian process regression to model the function <span class="math inline">\(f(x) = 5 \sin x + \sin 5 x\)</span> on the interval <span class="math inline">\([0, \pi]\)</span>, with i.i.d. Gaussian noise with standard deviation <span class="math inline">\(\sigma = 0.2\)</span> using twenty samples.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">5</span> <span class="op">*</span> np.sin(x) <span class="op">+</span> np.sin(<span class="dv">5</span> <span class="op">*</span> x)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>sample_xs <span class="op">=</span> sp.stats.uniform.rvs(scale<span class="op">=</span>np.pi, size<span class="op">=</span>n)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>sample_ys <span class="op">=</span> f(sample_xs) <span class="op">+</span> sp.stats.norm.rvs(scale<span class="op">=</span>sigma, size<span class="op">=</span>n)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="dv">0</span>, np.pi, <span class="dv">100</span>)</span></code></pre></div>
<center>
<img src="/resources/bayesian-nonparametric-gp/function.png" title="fig:" alt="png" />
</center>
<p>The following class implements this approach to Gaussian process regression as a sublcass of <code>scikit-learn</code>’s <a href="http://scikit-learn.org/dev/modules/generated/sklearn.base.BaseEstimator.html"><code>sklearn.base.BaseEstimator</code></a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPRegression(BaseEstimator):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, l<span class="op">=</span><span class="fl">1.0</span>, sigma<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l <span class="op">=</span> l</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigma <span class="op">=</span> sigma</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> confidence_band(<span class="va">self</span>, X, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Calculates pointwise confidence bands with coverage 1 - alpha</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> <span class="va">self</span>.mean(X)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        cov <span class="op">=</span> <span class="va">self</span>.cov(X)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> np.sqrt(np.diag(cov))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        upper <span class="op">=</span> mean <span class="op">+</span> std <span class="op">*</span> sp.stats.norm.isf(alpha <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        lower <span class="op">=</span> mean <span class="op">-</span> std <span class="op">*</span> sp.stats.norm.isf(alpha <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> lower, upper</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cov(<span class="va">self</span>, X):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> k(X, X, <span class="va">self</span>.l) <span class="op">-</span> np.dot(k(X, <span class="va">self</span>.X, <span class="va">self</span>.l), </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>                                 np.dot(np.linalg.inv(k(<span class="va">self</span>.X, <span class="va">self</span>.X, <span class="va">self</span>.l) </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>                                                      <span class="op">+</span> <span class="va">self</span>.sigma<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.eye(<span class="va">self</span>.X.size)),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>                                        k(<span class="va">self</span>.X, X, <span class="va">self</span>.l)))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> X</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> y</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mean(<span class="va">self</span>, X):</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(k(X, <span class="va">self</span>.X, <span class="va">self</span>.l),</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>               np.dot(np.linalg.inv(k(<span class="va">self</span>.X, <span class="va">self</span>.X, <span class="va">self</span>.l)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>                        <span class="op">+</span> <span class="va">self</span>.sigma<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.eye(<span class="va">self</span>.X.size)),</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>                      <span class="va">self</span>.y))</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mean(X)</span></code></pre></div>
<p>Subclassing <code>BaseEstimator</code> allows us to use <code>scikit-learn</code>’s <a href="http://scikit-learn.org/stable/model_selection.html">cross-validation tools</a> to choose the values of the hyperparameters <span class="math inline">\(\ell\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GPRegression()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&#39;l&#39;</span>: np.linspace(<span class="fl">0.01</span>, <span class="fl">0.6</span>, <span class="dv">50</span>),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">&#39;sigma&#39;</span>: np.linspace(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">50</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>              }</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> GridSearchCV(gp, param_grid, scoring<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>cv.fit(sample_xs, sample_ys)<span class="op">;</span></span></code></pre></div>
<p>Cross-validation yields the following parameters, which produce a relatively small mean squared error.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GPRegression(<span class="op">**</span>cv.best_params_)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model.fit(sample_xs, sample_ys)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>cv.best_params_</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;l&#39;</span>: <span class="fl">0.40734693877551015</span>, <span class="st">&#39;sigma&#39;</span>: <span class="fl">0.1020408163265306</span>}</span></code></pre></div>
<p>The figure below shows the true regression function, <span class="math inline">\(f(x) = 5 \sin x + \sin 5 x\)</span> along with our Gaussian process regression estimate. It also shows a 95% pointwise confidence band around the estimated regression function.</p>
<center>
<img src="/resources/bayesian-nonparametric-gp/fit.png" title="fig:" alt="png" />
</center>
<p>Note now near observed points, the confidence band shrinks drastically, while away from observed points it expands rapidly.</p>
<h4 id="section"></h4>
<h4 id="properties-of-gaussian-process-regression">Properties of Gaussian Process Regression</h4>
<p>As our toy example shows, Gaussian processes are powerful and flexible tools for regression. Although we have only just scratched the surface of this vast field, some remarks about the general properties and utility of Gaussian process regression are in order. For a much more extensive introduction to Gaussian process consult the excellent book <a href="http://www.gaussianprocess.org/gpml/"><em>Gaussian Processes for Machine Learning</em></a>, which is freely available online.</p>
<p>In general, Gaussian process regression is only effective when the covariance function and its parameters are appropriately chosen. Although we have used cross-validation to choose the parameter values in our toy example, for more serious problems, it is often much quicker to maximize the marginal likelihood directly by calculating its gradient.</p>
<p>Our implementation of Gaussian process regression is numerically unstable, as it directly inverts the matrix <span class="math inline">\(k(X, X) + \sigma^2 I\)</span>. A more stable approach is to calculate the <a href="http://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> of this matrix. Unfortuantely, calculating the Cholesky decomposition of an <span class="math inline">\(n\)</span>-dimensional matrix takes <span class="math inline">\(O(n^3)\)</span> time. In the context of Gaussian process regression, <span class="math inline">\(n\)</span> is the number of observations. If the number of training points is significant, the cubic complexity may be prohibitive. A number of approaches have been developed to approximate the results of Gaussian process regression with larger training sets. For more details on these approximations, consult Chapter 8 of <em>Gaussian Processes for Machine Learning</em>.</p>
<p>This post is <a href="/resources/bayesian-nonparametric-gp/notebook.ipynb">available</a> as an IPython notebook.</p>
