<meta name="title" content="Fitting a Simple Additive Model in Python" />
<meta name="tags" content="Statistics, Python" />
<meta name="date" content="2015-08-29" />
<meta name="has_math" content="true" /><style>
.dataframe * {border-color: #c0c0c0 !important;}
.dataframe th{background: #eee;}
.dataframe td{
    background: #fff;
    text-align: right; 
    min-width:5em;
}

/* Format summary rows */
.dataframe-summary-row tr:last-child,
.dataframe-summary-col td:last-child{
background: #eee;
    font-weight: 500;
}
</style>
<p>Recently, I have been learning about <a href="https://en.wikipedia.org/wiki/Generalized_additive_model">(generalized) additive models</a> by working through <a href="http://people.bath.ac.uk/sw283/">Simon Wood’s</a> <a href="http://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1584884746">book</a>. I have previously posted an IPython <a href="http://nbviewer.ipython.org/gist/AustinRochford/bb19863446d46dbcdc83">notebook</a> implementing the models from Chapter 3 of the book. In this post, I will show how to fit a simple additive model in Python in a bit more detail.</p>
<p>We will use a <a href="https://en.wikipedia.org/wiki/Lidar">LIDAR</a> dataset that is available on the <a href="http://www.stat.cmu.edu/~larry/all-of-nonpar/data.html">website</a> for Larry Wasserman’s book <a href="http://www.stat.cmu.edu/~larry/all-of-nonpar/"><em>All of Nonparametric Statistics</em></a>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> patsy</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels <span class="im">import</span> api <span class="im">as</span> sm</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;http://www.stat.cmu.edu/~larry/all-of-nonpar/=data/lidar.dat&#39;</span>,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                 sep<span class="op">=</span><span class="st">&#39; *&#39;</span>, engine<span class="op">=</span><span class="st">&#39;python&#39;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;std_range&#39;</span>] <span class="op">=</span> (df.<span class="bu">range</span> <span class="op">-</span> df.<span class="bu">range</span>.<span class="bu">min</span>()) <span class="op">/</span> df.<span class="bu">range</span>.ptp()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> df.shape[<span class="dv">0</span>]</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
range
</th>
<th>
logratio
</th>
<th>
std_range
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
390
</td>
<td>
-0.050356
</td>
<td>
0.000000
</td>
</tr>
<tr>
<th>
1
</th>
<td>
391
</td>
<td>
-0.060097
</td>
<td>
0.003030
</td>
</tr>
<tr>
<th>
2
</th>
<td>
393
</td>
<td>
-0.041901
</td>
<td>
0.009091
</td>
</tr>
<tr>
<th>
3
</th>
<td>
394
</td>
<td>
-0.050985
</td>
<td>
0.012121
</td>
</tr>
<tr>
<th>
4
</th>
<td>
396
</td>
<td>
-0.059913
</td>
<td>
0.018182
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>This data set is well-suited to additive modeling because the relationship between the variables is highly non-linear.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>blue <span class="op">=</span> sns.color_palette()[<span class="dv">0</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>ax.scatter(df.std_range, df.logratio, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">1.01</span>)<span class="op">;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Scaled range&#39;</span>)<span class="op">;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Log ratio&#39;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/additive/Fitting%20a%20Simple%20Additive%20Model%20in%20Python_7_0.png" title="fig:" alt="png" />
</center>
<p>An additive model represents the relationship between explanatory variables <span class="math inline">\(\mathbf{x}\)</span> and a response variable <span class="math inline">\(y\)</span> as a sum of smooth functions of the explanatory variables</p>
<p><span class="math display">\[y = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_k(x_k) + \varepsilon.\]</span></p>
<p>The smooth functions <span class="math inline">\(f_i\)</span> can be estimated using a variety of nonparametric techniques. Following Chapter 3 of Wood’s book, we will fit our additive model using penalized regression splines.</p>
<p>Since our LIDAR data set has only one explanatory variable, our additive model takes the form</p>
<p><span class="math display">\[y = \beta_0 + f(x) + \varepsilon.\]</span></p>
<p>We fit this model by minimizing the penalized residual sum of squares</p>
<p><span class="math display">\[PRSS = \sum_{i = 1}^n \left(y_i - \beta_0 - f(x_i)\right)^2 + \lambda \int_0^1 \left(f&#39;&#39;(x)\right)^2\ dx.\]</span></p>
<p>The penalty term</p>
<p><span class="math display">\[\int_0^1 \left(f&#39;&#39;(x)\right)^2\ dx\]</span></p>
<p>causes us to only choose less smooth functions if they fit the data much better. The smoothing parameter <span class="math inline">\(\lambda\)</span> controls the rate at which decreased smoothness is traded for a better fit.</p>
<p>In the penalized regression splines model, we must also choose basis functions <span class="math inline">\(\varphi_1, \varphi_2, \ldots, \varphi_k\)</span>, which we then use to express the smooth function <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[f(x) = \beta_1 \varphi_1(x) + \beta_2 \varphi_2(x) + \cdots + \beta_k \varphi_k(x).\]</span></p>
<p>With these basis functions in place, if we define <span class="math inline">\(\mathbf{x}_i = [1\ x_i\ \varphi_2(x_i)\ \cdots \varphi_k(x_i)]\)</span> and</p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix}
    \mathbf{x}_1 \\
    \vdots \\
    \mathbf{x}_n
 \end{bmatrix},\]</span></p>
<p>the model <span class="math inline">\(y_i = \beta_0 + f(x_i) + \varepsilon\)</span> can be rewritten as <span class="math inline">\(\mathbf{y} = \mathbf{X} \beta + \varepsilon\)</span>. It is tedious but not difficult to show that when <span class="math inline">\(f\)</span> is expressed as a linear combination of basis functions, there is always a <a href="https://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices">positive semidefinite</a> matrix <span class="math inline">\(\mathbf{S}\)</span> such that</p>
<p><span class="math display">\[\int_0^1 \left(f&#39;&#39;(x)\right)^2\ dx = \beta^{\intercal} \mathbf{S} \beta.\]</span></p>
<p>Since <span class="math inline">\(\mathbf{S}\)</span> is positive semidefinite, it has a square root <span class="math inline">\(\mathbf{B}\)</span> such that <span class="math inline">\(\mathbf{B}^{\intercal} \mathbf{B} = \mathbf{S}\)</span>. The penalized residual sum of squares objective function can then be written as</p>
<p><span class="math display">\[
\begin{align*}
    PRSS
        &amp; = (\mathbf{y} - \mathbf{X} \beta)^{\intercal} (\mathbf{y} - \mathbf{X} \beta) + \lambda \beta^{\intercal} \mathbf{B}^{\intercal} \mathbf{B} \beta
          = (\mathbf{\tilde{y}} - \mathbf{\tilde{X}} \beta)^{\intercal} (\mathbf{\tilde{y}} - \mathbf{\tilde{X}} \beta),
\end{align*}
\]</span></p>
<p>where</p>
<p><span class="math display">\[\mathbf{\tilde{y}} = \begin{bmatrix}
    \mathbf{y} \\
    \mathbf{0}_{k + 1}
\end{bmatrix}
\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{\tilde{X}} = \begin{bmatrix}
    \mathbf{X} \\
    \sqrt{\lambda}\ \mathbf{B}
\end{bmatrix}. 
\]</span></p>
<p>Therefore the augmented data matrices <span class="math inline">\(\mathbf{\tilde{y}}\)</span> and <span class="math inline">\(\mathbf{\tilde{X}}\)</span> allow us to express the penalized residual sum of squares for the original model as the residual sum of squares of the OLS model <span class="math inline">\(\mathbf{\tilde{y}} = \mathbf{\tilde{X}} \beta + \tilde{\varepsilon}\)</span>. This augmented model allows us to use widely available machinery for fitting OLS models to fit the additive model as well.</p>
<p>The last step before we can fit the model in Python is to choose the basis functions <span class="math inline">\(\varphi_i\)</span>. Again, following Chapter 3 of Wood’s book, we let</p>
<p><span class="math display">\[R(x, z) = \frac{1}{4} \left(\left(z - \frac{1}{2}\right)^2 - \frac{1}{12}\right) \left(\left(x - \frac{1}{2}\right)^2 - \frac{1}{12}\right) - \frac{1}{24} \left(\left(\left|x - z\right| - \frac{1}{2}\right)^4 - \frac{1}{2} \left(\left|x - z\right| - \frac{1}{2}\right)^2 + \frac{7}{240}\right).\]</span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> R(x, z):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((z <span class="op">-</span> <span class="fl">0.5</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">12</span>) <span class="op">*</span> ((x <span class="op">-</span> <span class="fl">0.5</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">12</span>) <span class="op">/</span> <span class="dv">4</span> <span class="op">-</span> ((np.<span class="bu">abs</span>(x <span class="op">-</span> z) <span class="op">-</span> <span class="fl">0.5</span>)<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> (np.<span class="bu">abs</span>(x <span class="op">-</span> z) <span class="op">-</span> <span class="fl">0.5</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">7</span> <span class="op">/</span> <span class="dv">240</span>) <span class="op">/</span> <span class="dv">24</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.frompyfunc(R, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> R_(x):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> R.outer(x, knots).astype(np.float64)</span></code></pre></div>
<p>Though this function is quite complicated, we will see that it has some very conveient properties. We must also choose a set of knots <span class="math inline">\(z_i\)</span> in <span class="math inline">\([0, 1]\)</span>, <span class="math inline">\(i = 1, 2, \ldots, q\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>knots <span class="op">=</span> df.std_range.quantile(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, q))</span></code></pre></div>
<p>Here we have used twenty knots situatied at percentiles of <code>std_range</code>.</p>
<p>Now we define our basis functions as <span class="math inline">\(\varphi_1(x) = x\)</span>, <span class="math inline">\(\varphi_{i}(x) = R(x, z_{i - 1})\)</span> for <span class="math inline">\(i = 2, 3, \ldots q + 1\)</span>.</p>
<p>Our model matrices <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are therefore</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>y, X <span class="op">=</span> patsy.dmatrices(<span class="st">&#39;logratio ~ std_range + R_(std_range)&#39;</span>, data<span class="op">=</span>df)</span></code></pre></div>
<p>Note that, by default, <code>patsy</code> always includes an intercept column in <code>X</code>.</p>
<p>The advantage of the function <span class="math inline">\(R\)</span> is the the penalty matrix <span class="math inline">\(\mathbf{S}\)</span> has the form</p>
<p><span class="math display">\[S = \begin{bmatrix}
    \mathbf{0}_{2 \times 2} &amp; \mathbf{0}_{2 \times q} \\
    \mathbf{0}_{q \times 2} &amp; \mathbf{\tilde{S}}
\end{bmatrix},\]</span></p>
<p>where <span class="math inline">\(\mathbf{\tilde{S}}_{ij} = R(z_i, z_j)\)</span>. We now calculate <span class="math inline">\(\mathbf{S}\)</span> and its square root <span class="math inline">\(\mathbf{B}\)</span>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.zeros((q <span class="op">+</span> <span class="dv">2</span>, q <span class="op">+</span> <span class="dv">2</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>S[<span class="dv">2</span>:, <span class="dv">2</span>:] <span class="op">=</span> R_(knots)</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.zeros_like(S)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>B[<span class="dv">2</span>:, <span class="dv">2</span>:] <span class="op">=</span> np.real_if_close(sp.linalg.sqrtm(S[<span class="dv">2</span>:, <span class="dv">2</span>:]), tol<span class="op">=</span><span class="dv">10</span><span class="op">**</span><span class="dv">8</span>)</span></code></pre></div>
<p>We now have all the ingredients necessary to fit some additive models to the LIDAR data set.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(y, X, B, lambda_<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># build the augmented matrices</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    y_ <span class="op">=</span> np.vstack((y, np.zeros((q <span class="op">+</span> <span class="dv">2</span>, <span class="dv">1</span>))))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    X_ <span class="op">=</span> np.vstack((X, np.sqrt(lambda_) <span class="op">*</span> B))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sm.OLS(y_, X_).fit()</span></code></pre></div>
<p>We have not yet discussed how to choose the smoothing parameter <span class="math inline">\(\lambda\)</span>, so we will fit several models with different values of <span class="math inline">\(\lambda\)</span> to see how it affects the results.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">3</span>, ncols<span class="op">=</span><span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>, squeeze<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="fl">13.5</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plot_lambdas <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>, <span class="fl">0.00001</span>])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plot_x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plot_X <span class="op">=</span> patsy.dmatrix(<span class="st">&#39;std_range + R_(std_range)&#39;</span>, {<span class="st">&#39;std_range&#39;</span>: plot_x})</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lambda_, ax <span class="kw">in</span> <span class="bu">zip</span>(plot_lambdas, np.ravel(axes)):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    ax.scatter(df.std_range, df.logratio, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> fit(y, X, B, lambda_<span class="op">=</span>lambda_)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    ax.plot(plot_x, results.predict(plot_X))<span class="op">;</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">1.01</span>)<span class="op">;</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">&#39;Scaled range&#39;</span>)<span class="op">;</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">&#39;Log ratio&#39;</span>)<span class="op">;</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="vs">r&#39;$\lambda = </span><span class="sc">{}</span><span class="vs">$&#39;</span>.<span class="bu">format</span>(lambda_))<span class="op">;</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/additive/Fitting%20a%20Simple%20Additive%20Model%20in%20Python_20_0.png" width=800>
</center>
<p>We can see that as <span class="math inline">\(\lambda\)</span> decreases, the model becomes less smooth. Visually, it seems that the optimal value of <span class="math inline">\(\lambda\)</span> lies somewhere between <span class="math inline">\(10^{-2}\)</span> and <span class="math inline">\(10^{-4}\)</span>. We need a rigorous way to choose the optimal value of <span class="math inline">\(\lambda\)</span>. As is often the case in such situations, we turn to <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>. Specifically, we will use <a href="http://www.stat.wisc.edu/~wahba/ftp1/oldie/golub.heath.wahba.pdf">generalized cross-validation</a> to choose the optimal value of <span class="math inline">\(\lambda\)</span>. The GCV score is given by</p>
<p><span class="math display">\[\operatorname{GCV}(\lambda) = \frac{n \sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2}{\left(n - \operatorname{tr} \mathbf{H}\right)^2}.\]</span></p>
<p>Here, <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i\)</span>-th predicted value, and <span class="math inline">\(\mathbf{H}\)</span> is upper left <span class="math inline">\(n \times n\)</span> submatrix of the the <a href="https://en.wikipedia.org/wiki/Hat_matrix">influence matrix</a> for the OLS model <span class="math inline">\(\mathbf{\tilde{y}} = \mathbf{\tilde{X}} + \varepsilon\)</span>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gcv_score(results):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> results.model.exog[:<span class="op">-</span>(q <span class="op">+</span> <span class="dv">2</span>), :]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> results.model.endog[:n]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> results.predict(X)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    hat_matrix_trace <span class="op">=</span> results.get_influence().hat_matrix_diag[:n].<span class="bu">sum</span>()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n <span class="op">*</span> np.power(y <span class="op">-</span> y_hat, <span class="dv">2</span>).<span class="bu">sum</span>() <span class="op">/</span> np.power(n <span class="op">-</span> hat_matrix_trace, <span class="dv">2</span>)</span></code></pre></div>
<p>Now we evaluate the GCV score of the model over a range of <span class="math inline">\(\lambda\)</span> values.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>lambdas <span class="op">=</span> np.logspace(<span class="dv">0</span>, <span class="dv">50</span>, <span class="dv">100</span>, base<span class="op">=</span><span class="fl">1.5</span>) <span class="op">*</span> <span class="fl">1e-8</span></span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>gcv_scores <span class="op">=</span> np.array([gcv_score(fit(y, X, B, lambda_<span class="op">=</span>lambda_)) <span class="cf">for</span> lambda_ <span class="kw">in</span> lambdas])</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>ax.plot(lambdas, gcv_scores)<span class="op">;</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>ax.set_xscale(<span class="st">&#39;log&#39;</span>)<span class="op">;</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r&#39;$\lambda$&#39;</span>)<span class="op">;</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&#39;$\operatorname</span><span class="sc">{GCV}</span><span class="vs">(\lambda)$&#39;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/additive/Fitting%20a%20Simple%20Additive%20Model%20in%20Python_26_0.png" title="fig:" alt="png" />
</center>
<p>The GCV-optimal value of <span class="math inline">\(\lambda\)</span> is therefore</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>lambda_best <span class="op">=</span> lambdas[gcv_scores.argmin()]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>lambda_best</span></code></pre></div>
<pre><code>0.00063458365729550153</code></pre>
<p>This value of <span class="math inline">\(\lambda\)</span> produces a visually reasonable fit.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ax.scatter(df.std_range, df.logratio, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> fit(y, X, B, lambda_<span class="op">=</span>lambda_best)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>ax.plot(plot_x, results.predict(plot_X), label<span class="op">=</span><span class="vs">r&#39;$\lambda = </span><span class="sc">{}</span><span class="vs">$&#39;</span>.<span class="bu">format</span>(lambda_best))<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">1.01</span>)<span class="op">;</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>ax.legend()<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/additive/Fitting%20a%20Simple%20Additive%20Model%20in%20Python_30_0.png" title="fig:" alt="png" />
</center>
<p>We have only scratched the surface of additive models, fitting a simple model of one variable with penalized regression splines. In general, additive models are quite powerful and flexible, while remaining quite interpretable.</p>
<p>This post is available as an <a href="http://ipython.org/">IPython</a> notebook <a href="http://nbviewer.ipython.org/gist/AustinRochford/c9c6862c6f7f7a28870a">here</a>.</p>
