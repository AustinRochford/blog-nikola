<meta name="title" content="Bayesian Factor Analysis Regression in Python with PyMC3" />
<meta name="tags" content="PyMC3, Bayesian, Python" />
<meta name="date" content="2021-07-05" />
<meta name="has_math" content="true" /><p>Wikipedia defines <a href="https://en.wikipedia.org/wiki/Factor_analysis">factor analysis</a> as</p>
<blockquote>
<p>a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.</p>
</blockquote>
<p>Factor analysis is used frequently in fields such as psychometrics, political science, and many others to identify latent (unmeasurable) traits that influence the observable behavior of people and systems.</p>
<p>Mathematically, <a href="https://probml.github.io/pml-book/book0.html"><em>Machine Learning: A Probabilistic Perspective</em></a> (my favorite ML reference) describes factor analysis as “a low rank parametrization of [a multivariate normal distribution]” and as “a way of specifying a joint density model on [the vector] <span class="math inline">\(\mathbf{x}\)</span>” using a small number of parameters.</p>
<p>This post will show how to add a richer covariance structure to the analysis of a simulated multivariate regression problem using factor analysis in Python with PyMC3. As we will see, specifying this model is somewhat tricky due to <a href="https://en.wikipedia.org/wiki/Identifiability">identifiability</a> issues with naive model specifications.</p>
<h2 id="simulated-data">Simulated data</h2>
<p>We begin by simulating the data that we will subsequently analyze. First we make the necessary python imports and do some light housekeeping.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> warnings <span class="im">import</span> filterwarnings</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aesara <span class="im">import</span> tensor <span class="im">as</span> at</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc3 <span class="im">as</span> pm</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code></pre></div>
<pre><code>You are running the v4 development version of PyMC3 which currently still lacks key features. You probably want to use the stable v3 instead which you can either install via conda or find on the v3 GitHub branch: https://github.com/pymc-devs/pymc3/tree/v3</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span><span class="pp">RuntimeWarning</span>, module<span class="op">=</span><span class="st">&#39;arviz&#39;</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span><span class="pp">UserWarning</span>, module<span class="op">=</span><span class="st">&#39;arviz&#39;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span><span class="pp">UserWarning</span>, module<span class="op">=</span><span class="st">&#39;pandas&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">&#39;figure.figsize&#39;</span>] <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">6</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(color_codes<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>The observed predictors, <span class="math inline">\(\mathbf{x}_i\)</span>, will be <span class="math inline">\(5\)</span>-dimensional vectors with entries drawn i.i.d. from the standard normal distribution. We generate 100 such vectors.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">123456789</span> <span class="co"># for reproducibility</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(SEED)</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> rng.normal(size<span class="op">=</span>(N, K))</span></code></pre></div>
<p>The observed targets, <span class="math inline">\(\mathbf{y}_i\)</span>, will be 10-dimensional vectors.</p>
<p>The constant term for this model, <span class="math inline">\(B_0\)</span>, is therefore a random vector in <span class="math inline">\(\mathbb{R}^{10}\)</span> whose entries are i.i.d. drawn from the uniform distribution on <span class="math inline">\([-3, 3]\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">10</span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>B0 <span class="op">=</span> rng.uniform(<span class="op">-</span><span class="fl">3.</span>, <span class="fl">3.</span>, size<span class="op">=</span>M)</span></code></pre></div>
<p>The component slopes form a <span class="math inline">\(10 \times 5\)</span> dimensional matrix. We generate a somewhat sparse matrix that has only (approximately) 50% of entries nonzero. These entries are also i.i.d. from the uniform distribution on <span class="math inline">\([-3, 3]\)</span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.zeros((M, K))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>P_nonzero <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>is_nonzero <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, P_nonzero, size<span class="op">=</span>(M, K)) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>B[is_nonzero] <span class="op">=</span> rng.uniform(<span class="op">-</span><span class="fl">3.</span>, <span class="fl">3.</span>, size<span class="op">=</span>is_nonzero.<span class="bu">sum</span>())</span></code></pre></div>
<p>The observations before noise are</p>
<p><span class="math display">\[\mathbf{y}_i^{\text{noiseless}} = B_0 + B\ \mathbf{x}_i.\]</span></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>Y_noiseless <span class="op">=</span> B0 <span class="op">+</span> X.dot(B.T)</span></code></pre></div>
<p>So far this is a fairly standard simulation from a linear regression model. Things start to get more interesting when we introduce a non-diagonal correlation structure to the noise using latent factors. The noise added to the <span class="math inline">\(j\)</span>-th component of the <span class="math inline">\(i\)</span>-th sample is</p>
<p><span class="math display">\[\varepsilon_{i, j} = \mathbf{w}_j \cdot \mathbf{z}_i + \sigma_{i, j}\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}_1, \cdots \mathbf{w}_{10}\)</span>, and <span class="math inline">\(\mathbf{z}_1, \ldots, \mathbf{z}_{100}\)</span> are vectors in <span class="math inline">\(\mathbb{R}^2\)</span> whose entries are drawn i.i.d. from a standard normal distribution. Here two is the number of latent factors that govern the covariance structure. The uncorrelated noise is drawn i.i.d. from a normal distribution with variance <span class="math inline">\((0.25)^2\)</span>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>F <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>S_SCALE <span class="op">=</span> <span class="fl">1.</span></span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> rng.normal(size<span class="op">=</span>(M, F))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> rng.normal(size<span class="op">=</span>(N, F))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> rng.normal(scale<span class="op">=</span>S_SCALE, size<span class="op">=</span>(N, M))</span></code></pre></div>
<p>We now add this noise to our noiseless “data” to produce the actual observations.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> Y_noiseless <span class="op">+</span> Z.dot(W.T) <span class="op">+</span> S</span></code></pre></div>
<p>Plotting the components of <span class="math inline">\(\mathbf{x}_i\)</span> against those of <span class="math inline">\(\mathbf{y}_i\)</span>, we see (somewhat) sparse linear patterns emerge.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span>M, ncols<span class="op">=</span>K,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                         sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                         figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">12</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j, row_axes <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(row_axes):</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[:, i], Y[:, j], alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="ss">f&quot;$x_</span><span class="ch">{{</span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">$&quot;</span>)<span class="op">;</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="ss">f&quot;$y_</span><span class="ch">{{</span><span class="sc">{</span>j <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">$&quot;</span>)<span class="op">;</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_25_0.png" title="fig:" alt="png" />
</center>
<h2 id="modeling">Modeling</h2>
<p>We now begin building a series of models of this data using PyMC3.</p>
<h3 id="no-latent-factors">No latent factors</h3>
<p>We start with a simple multivariate regression model that assumes diagonal covariance to see the impact of ignoring the latent factors on our model.</p>
<p>We place i.i.d. <span class="math inline">\(N(0, 5^2)\)</span> priors on the components of the intercepts (<span class="math inline">\(\beta_0\)</span>) and the slopes (<span class="math inline">\(\beta\)</span>).</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> multi_model:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    β<span class="dv">0</span> <span class="op">=</span> pm.Normal(<span class="st">&quot;β0&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>M)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    β <span class="op">=</span> pm.Normal(<span class="st">&quot;β&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>(M, K))</span></code></pre></div>
<p>The noise scale gets the prior <span class="math inline">\(\sigma \sim \text{Half}-N(2.5^2)\)</span>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> multi_model:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    σ <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;σ&quot;</span>, <span class="fl">2.5</span>)</span></code></pre></div>
<p>We now specify the likelihood of the observed data given the covariates.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> multi_model:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> β<span class="dv">0</span> <span class="op">+</span> at.dot(X, β.T)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    obs <span class="op">=</span> pm.Normal(<span class="st">&quot;obs&quot;</span>, μ, σ, observed<span class="op">=</span>Y)</span></code></pre></div>
<p>We now sample from the posterior distribution of these parameters.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>CORES <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>SAMPLE_KWARGS <span class="op">=</span> {</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;cores&#39;</span>: CORES,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;init&#39;</span>: <span class="st">&#39;adapt_diag&#39;</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;random_seed&#39;</span>: [SEED <span class="op">+</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(CORES)],</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;return_inferencedata&#39;</span>: <span class="va">True</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> multi_model:</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    multi_trace <span class="op">=</span> pm.sample(<span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [β0, β, σ]</code></pre>
<div>
<style>
/* Turns off some styling */
progress {
/* gets rid of default border in Firefox and Opera. */
border: none;
/* Needs to be in here for Safari polyfill so background images work as expected. */
background-size: auto;
}
.progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
background: #F44336;
}
</style>
<progress value="8000" class max="8000" style="width:300px; height:20px; vertical-align: middle;">
</progress>
<p>100.00% [8000/8000 00:50&lt;00:00 Sampling 4 chains, 0 divergences]</p>
</div>
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 56 seconds.</code></pre>
<p>Standard sampling diagnostics (energy plots, BFMI, and <span class="math inline">\(\hat{R}\)</span>) show no cause for concern.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_diagnostic_plots(trace, axes<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                          min_mult<span class="op">=</span><span class="fl">0.995</span>, max_mult<span class="op">=</span><span class="fl">1.005</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                          var_names<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> axes <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                                 sharex<span class="op">=</span><span class="va">False</span>, sharey<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>                                 figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    az.plot_energy(trace, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    rhat <span class="op">=</span> az.rhat(trace, var_names<span class="op">=</span>var_names).<span class="bu">max</span>()</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].barh(np.arange(<span class="bu">len</span>(rhat.variables)), rhat.to_array(),</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                 tick_label<span class="op">=</span><span class="bu">list</span>(rhat.variables.keys()))</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].axvline(<span class="dv">1</span>, c<span class="op">=</span><span class="st">&#39;k&#39;</span>, ls<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_xlim(</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        min_mult <span class="op">*</span> <span class="bu">min</span>(rhat.<span class="bu">min</span>().to_array().<span class="bu">min</span>(), <span class="dv">1</span>),</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        max_mult <span class="op">*</span> <span class="bu">max</span>(rhat.<span class="bu">max</span>().to_array().<span class="bu">max</span>(), <span class="dv">1</span>)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_xlabel(<span class="vs">r&quot;$\hat</span><span class="sc">{R}</span><span class="vs">$&quot;</span>)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].set_ylabel(<span class="st">&quot;Variable&quot;</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig, axes</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>make_diagnostic_plots(multi_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_38_0.png" title="fig:" alt="png" />
</center>
<p>The following plot shows excellent agreement between the actual and estimated values of the intercepts.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>ax, <span class="op">=</span> az.plot_forest(multi_trace, var_names<span class="op">=</span><span class="st">&quot;β0&quot;</span>, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>ax.scatter([], [],</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>           facecolors<span class="op">=</span><span class="st">&#39;none&#39;</span>, edgecolors<span class="op">=</span><span class="st">&#39;C0&#39;</span>,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span>)<span class="op">;</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(B0, ax.get_yticks()[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>           c<span class="op">=</span><span class="st">&#39;k&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Actual&quot;</span>)<span class="op">;</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])<span class="op">;</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\beta_0$&quot;</span>)<span class="op">;</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">&quot;upper left&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_40_0.png" title="fig:" alt="png" />
</center>
<p>There is similarly excellent agreement for the component coefficients.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>ax, <span class="op">=</span> az.plot_forest(multi_trace, var_names<span class="op">=</span><span class="st">&quot;β&quot;</span>,</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>                     coords<span class="op">=</span>{<span class="st">&quot;β_dim_1&quot;</span>: <span class="dv">0</span>},</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>                     hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>ax.scatter([], [],</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>           facecolors<span class="op">=</span><span class="st">&#39;none&#39;</span>, edgecolors<span class="op">=</span><span class="st">&#39;C0&#39;</span>,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span>)<span class="op">;</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>ax.scatter(B[:, <span class="dv">0</span>], ax.get_yticks()[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>           c<span class="op">=</span><span class="st">&#39;k&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Actual&quot;</span>)<span class="op">;</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])<span class="op">;</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\beta_{i, 0}$&quot;</span>)<span class="op">;</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">&quot;upper left&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_42_0.png" title="fig:" alt="png" />
</center>
<p>Note that here we have only plotted the posterior distributions for the first column of <span class="math inline">\(\beta\)</span>. The results are similar for the other columns.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(multi_trace, var_names<span class="op">=</span><span class="st">&quot;σ&quot;</span>, ref_val<span class="op">=</span>S_SCALE)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_44_0.png" title="fig:" alt="png" />
</center>
<p>As we can see, the posterior estimate of <span class="math inline">\(\sigma\)</span> is much higher than the true uncorrelated noise scale of one. This is due to the fact that we have not modeled the correlated noise induced by the latent factors. Indeed, since</p>
<p><span class="math display">\[\varepsilon_{i, j} = \mathbf{w}_j \cdot \mathbf{z}_i + \sigma_{i, j},\]</span></p>
<p>we get</p>
<p><span class="math display">\[
\begin{align*}
    \operatorname{Var}(\varepsilon_{i, j})
        &amp; = \operatorname{Var}(\mathbf{w}_j \cdot \mathbf{z}_i + \sigma_{i, j}) \\
        &amp; = 2 \operatorname{Var}(w_{i, j}) \cdot \operatorname{Var}(z_{i, j}) + 1^2 \\
        &amp; = 3,
\end{align*}
\]</span></p>
<p>since <span class="math inline">\(w_{i, j}\)</span> and <span class="math inline">\(z_{i, j}\)</span> are independent standard normal variables, which are also independent of <span class="math inline">\(\sigma_{i, j}\)</span>. Not accounting for the variation induced by the <span class="math inline">\(\mathbf{w}_j \cdot \mathbf{z}_i\)</span> term in this model has caused the estimate of the scale of <span class="math inline">\(\sigma\)</span> to be inflated.</p>
<h3 id="with-latent-factors">With latent factors</h3>
<p>We now add latent factors to our model, starting with the most straightforward parametrization. The priors on <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> are the same as for the previous model.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> factor_model:</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    β<span class="dv">0</span> <span class="op">=</span> pm.Normal(<span class="st">&quot;β0&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>M)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    β <span class="op">=</span> pm.Normal(<span class="st">&quot;β&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>(M, K))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> β<span class="dv">0</span> <span class="op">+</span> at.dot(X, β.T)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    σ <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;σ&quot;</span>, <span class="fl">2.5</span>)</span></code></pre></div>
<p>We place i.i.d. standard normal priors on the entries of <span class="math inline">\(\mathbf{w}_j\)</span> and <span class="math inline">\(\mathbf{z}_i\)</span>. and add their product to the expected value of observations.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> factor_model:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> pm.Normal(<span class="st">&quot;w&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(M, F))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> pm.Normal(<span class="st">&quot;z&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(N, F))</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    obs <span class="op">=</span> pm.Normal(<span class="st">&quot;obs&quot;</span>, μ <span class="op">+</span> z.dot(w.T), σ, observed<span class="op">=</span>Y)</span></code></pre></div>
<p>Again we sample from the posterior distribution of this model.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>START_MULT <span class="op">=</span> <span class="fl">3.</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>START_CORNERS <span class="op">=</span> np.array([</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span>],</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span>]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>w_starts <span class="op">=</span> [</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;w&quot;</span>: w_start, <span class="st">&quot;z&quot;</span>: np.ones((N, F))} <span class="cf">for</span> w_start</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="kw">in</span> START_MULT <span class="op">*</span> np.ones((<span class="dv">1</span>, M, F)) <span class="op">*</span> START_CORNERS[:, np.newaxis]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> factor_model:</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    factor_trace <span class="op">=</span> pm.sample(start<span class="op">=</span>w_starts, <span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [β0, β, σ, w, z]</code></pre>
<div>
<style>
/* Turns off some styling */
progress {
/* gets rid of default border in Firefox and Opera. */
border: none;
/* Needs to be in here for Safari polyfill so background images work as expected. */
background-size: auto;
}
.progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
background: #F44336;
}
</style>
<progress value="8000" class max="8000" style="width:300px; height:20px; vertical-align: middle;">
</progress>
<p>100.00% [8000/8000 03:11&lt;00:00 Sampling 4 chains, 0 divergences]</p>
</div>
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 193 seconds.
The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.
The estimated number of effective samples is smaller than 200 for some parameters.</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> statistics for <span class="math inline">\(\mathbf{z}_i\)</span> and <span class="math inline">\(\mathbf{w}_j\)</span> are quite high and warrant investigation.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>make_diagnostic_plots(factor_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_54_0.png" title="fig:" alt="png" />
</center>
<p>In spite of the large <span class="math inline">\(\hat{R}\)</span> statistics, the estimate of the scale of <span class="math inline">\(\sigma\)</span> is quite accurate for this model, since we have accounted for the noise due to the latent variables.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(factor_trace, var_names<span class="op">=</span><span class="st">&quot;σ&quot;</span>, ref_val<span class="op">=</span>S_SCALE)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_56_0.png" title="fig:" alt="png" />
</center>
<p>We turn to the posterior distributions of the factor loadings <span class="math inline">\(\mathbf{w}_j\)</span>. Below we plot the first two entries in <span class="math inline">\(\mathbf{w}_0\)</span> and <span class="math inline">\(\mathbf{w}_1\)</span> against each other.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> az.plot_pair(factor_trace, var_names<span class="op">=</span><span class="st">&quot;w&quot;</span>,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                  coords<span class="op">=</span>{<span class="st">&quot;w_dim_0&quot;</span>: [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">&quot;w_dim_1&quot;</span>: <span class="dv">0</span>},</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                  scatter_kwargs<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.5</span>})</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;$w_{0, 0}$&quot;</span>)<span class="op">;</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&quot;$w_{1, 0}$&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_58_0.png" title="fig:" alt="png" />
</center>
<p>The elliptical shape of this posterior distribution is certainly unusual; let’s see what the pairwise plots look like for all pairs of entries in the factor loadings.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> az.plot_pair(factor_trace, var_names<span class="op">=</span><span class="st">&quot;w&quot;</span>,</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>                   scatter_kwargs<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.5</span>})</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes.flat:</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="va">None</span>)<span class="op">;</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="va">None</span>)<span class="op">;</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].figure.tight_layout()<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_60_0.png" title="fig:" alt="png" />
</center>
<p>The shape of these posterior plots are odd indeed. These ellipses arise from the fact that the likelihood of the observed data is invariant under rotations of the latent two-dimensional space that the <span class="math inline">\(\mathbf{w}_j\)</span>s live in.</p>
<p>To demonstrate this rotation invariance recall that</p>
<p><span class="math display">\[y_{i, j}\ |\ \mathbf{x}_i, \mathbf{w}_j, \mathbf{z}_i, \beta_0, \beta, \sigma \sim N\left(\beta_0 + \beta_j \cdot \mathbf{x}_i + \mathbf{w}_j \cdot \mathbf{z}_i, \sigma^2\right).\]</span></p>
<p>Let <span class="math inline">\(R \in \mathbb{R}^{2 \times 2}\)</span> be a rotation matrix, so <span class="math inline">\(R^{\top} R = I_2\)</span>. Define <span class="math inline">\(\tilde{\mathbf{w}}_j = R \mathbf{w}_j\)</span> and <span class="math inline">\(\tilde{\mathbf{z}}_i = R \mathbf{z}_i\)</span>, so</p>
<p><span class="math display">\[
\begin{align*}
    \tilde{\mathbf{w}}_j \cdot \tilde{\mathbf{z}}_i
        &amp; = \tilde{\mathbf{w}}_j^{\top} \tilde{\mathbf{z}}_i \\
        &amp; = \mathbf{w}_j^{\top} R^{\top} R \mathbf{z}_i \\
        &amp; = \mathbf{w}_j^{\top} \mathbf{z}_i \\
        &amp; = \mathbf{w}_j \cdot \mathbf{z}_i.
\end{align*}
\]</span></p>
<p>Because rotation by <span class="math inline">\(R\)</span> preserves dot products, <span class="math inline">\(y_{i, j}\ |\ \mathbf{x}_i, \mathbf{w}_j, \mathbf{z}_i, \beta_0, \beta, \sigma\)</span> and <span class="math inline">\(y_{i, j}\ |\ \mathbf{x}_i, \tilde{\mathbf{w}}_j, \tilde{\mathbf{z}}_i, \beta_0, \beta, \sigma\)</span> have the same distribution.</p>
<h4 id="breaking-rotation-invariance">Breaking rotation invariance</h4>
<p>There are a number of ways to deal with the rotational invariance of this model’s naively parametrized likelihood. Those interested in maximum likelihood (MLE) or regularized maximum likelihood (maxium a priori/MAP) estimation usually don’t worry about rotational invariance as all rotations should produce the same likelihood. In this case practitioners may use whichever solution their gradient descent algorithm converges to, or they may choose an informative rotation based on their application. In Bayesian MCMC estimation, sometimes the non-identification is embraced and a rotation is fixed after sampling. I tend to find such approaches unsatisfying, as they render our convergence checks less useful than they are for a fully identified model.</p>
<p>Fortunately a <a href="https://arxiv.org/abs/1909.08022">paper</a> of Peeters<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> gives sufficient conditions for priors on <span class="math inline">\(\mathbf{w}_j\)</span> to break this rotational symmetry. With <span class="math inline">\(N\)</span> observations, let</p>
<p><span class="math display">\[
\begin{align*}
    W
        &amp; = \begin{pmatrix}
            \mathbf{w}_1 \\
            \vdots \\
            \mathbf{w}_N
        \end{pmatrix}
\end{align*}
\]</span></p>
<p>be the matrix with the loading factors as its rows. For factors of latent dimension <span class="math inline">\(F\)</span>, Peeter’s conditions are</p>
<ol type="1">
<li><span class="math inline">\(W\)</span> has at least <span class="math inline">\(F - 1\)</span> fixed zero entries in each column.</li>
<li>For each column, the rank of the of the submatrix that results from deleting the rows where that column has fixed zeros should be <span class="math inline">\(M - 1\)</span>.</li>
<li>The covariance matrix of the <span class="math inline">\(\mathbf{z}_i\)</span>s is one on the diagonal.</li>
<li>At least one value in each column is constrained to either be positive or negative only.</li>
</ol>
<p>Conditions 1, 3, and 4 are easy enough to satisfy as we will see. After a bit of reflection, condition 2 is also <a href="https://en.wikipedia.org/wiki/Almost_everywhere">almost always</a> satisfied for the following technical reason. Assume the matrix, <span class="math inline">\(X\)</span>, is square of dimension <span class="math inline">\(N \times N\)</span> and not invertible. We must have <span class="math inline">\(\operatorname{det} X = 0\)</span>, which means that the set of invertible matrices form an <span class="math inline">\(N^2 - 1\)</span> dimensional manifold in the <span class="math inline">\(N^2\)</span> dimensional space <span class="math inline">\(\mathbb{R}^{N \times N}\)</span>. Since every set of less than full dimension has <a href="https://en.wikipedia.org/wiki/Lebesgue_measure">Lebesgue measure</a> zero, as long as we choose a random matrix from a reasonable distribution (one that is <a href="https://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures">absolutely continuous</a> with respect to the Lebesgue measure) it will almost certainly not have determinant zero and therefore have full rank. More technical arguments show that this is also true of non-square matrices drawn from reasonable distributions.</p>
<p>We now build a new representation of this model that satisfies these four conditions for breaking rotational invariance. The priors on <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(\mathbf{z}_i\)</span> are the same as for the previous model.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> rot_model:</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    β<span class="dv">0</span> <span class="op">=</span> pm.Normal(<span class="st">&quot;β0&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>M)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    β <span class="op">=</span> pm.Normal(<span class="st">&quot;β&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>(M, K))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> β<span class="dv">0</span> <span class="op">+</span> at.dot(X, β.T)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    σ <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;σ&quot;</span>, <span class="fl">2.5</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> pm.Normal(<span class="st">&quot;z&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(N, F))</span></code></pre></div>
<p>For our two-dimensional example, we will build a factor loading matrix, <span class="math inline">\(W\)</span>, of the following form in stages.</p>
<p><span class="math display">\[
W =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
+ &amp; + \\
? &amp; ? \\
\vdots &amp; \vdots \\
? &amp; ?
\end{pmatrix}
\]</span></p>
<p>Here <span class="math inline">\(+\)</span> means that the entry is constrained to be positive and <span class="math inline">\(?\)</span> means that entry is unconstrained. First we set the first two rows to be the two-by-two identity matrix.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>w_top <span class="op">=</span> at.eye(M, F)</span></code></pre></div>
<p>Next we add the row of entries constrained to be positive below that.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>HALFNORMAL_SCALE <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> np.sqrt(<span class="fl">1.</span> <span class="op">-</span> <span class="fl">2.</span> <span class="op">/</span> np.pi)</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rot_model:</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    w_top_pos <span class="op">=</span> at.set_subtensor(</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>        w_top[F],</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>        pm.HalfNormal(<span class="st">&quot;w_pos&quot;</span>, HALFNORMAL_SCALE, shape<span class="op">=</span>F)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>Note that <code>HALFNORMAL_SCALE</code> is the <a href="https://en.wikipedia.org/wiki/Half-normal_distribution#Properties">scale necessary</a> for the entries in this row to have variance one.</p>
<p>Finally, we add the bottom block of unconstrained entries.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rot_model:</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> pm.Deterministic(<span class="st">&quot;w&quot;</span>, at.set_subtensor(</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>        w_top_pos[F <span class="op">+</span> <span class="dv">1</span>:],</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>        pm.Normal(<span class="st">&quot;w_block&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(M <span class="op">-</span> (F <span class="op">+</span> <span class="dv">1</span>), F))</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    ))</span></code></pre></div>
<p>Evaluating <code>w</code> shows that it has the desired structure.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>w.<span class="bu">eval</span>()</span></code></pre></div>
<pre><code>array([[ 1.        ,  0.        ],
       [ 0.        ,  1.        ],
       [ 1.20384406,  4.14310473],
       [-0.14646375, -0.84303061],
       [ 1.10983887,  0.49052328],
       [-1.89554466,  0.67558106],
       [-0.96375033,  0.02590003],
       [-0.10067908,  0.25301074],
       [-0.97963568, -1.44020378],
       [-0.48985021,  0.40814108]])</code></pre>
<p>The likelihood of the observed data is the same as in the previous model.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rot_model:</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    obs <span class="op">=</span> pm.Normal(<span class="st">&quot;obs&quot;</span>, μ <span class="op">+</span> z.dot(w.T), σ, observed<span class="op">=</span>Y)</span></code></pre></div>
<p>Again we sample from the posterior distribution of this model.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>w_block_starts <span class="op">=</span> [</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;w_block&quot;</span>: w_start, <span class="st">&quot;z&quot;</span>: np.ones((N, F))} <span class="cf">for</span> w_start</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        <span class="kw">in</span> START_MULT <span class="op">*</span> np.ones((<span class="dv">1</span>, M <span class="op">-</span> (F <span class="op">+</span> <span class="dv">1</span>), F)) <span class="op">*</span> START_CORNERS[:, np.newaxis]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rot_model:</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    rot_trace <span class="op">=</span> pm.sample(start<span class="op">=</span>w_block_starts, <span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [β0, β, σ, z, w_pos, w_block]</code></pre>
<div>
<style>
/* Turns off some styling */
progress {
/* gets rid of default border in Firefox and Opera. */
border: none;
/* Needs to be in here for Safari polyfill so background images work as expected. */
background-size: auto;
}
.progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
background: #F44336;
}
</style>
<progress value="8000" class max="8000" style="width:300px; height:20px; vertical-align: middle;">
</progress>
<p>100.00% [8000/8000 02:55&lt;00:00 Sampling 4 chains, 320 divergences]</p>
</div>
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 176 seconds.
There were 320 divergences after tuning. Increase `target_accept` or reparameterize.
The acceptance probability does not match the target. It is 0.5597958935132601, but should be close to 0.8. Try to increase the number of tuning steps.
The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.
The estimated number of effective samples is smaller than 200 for some parameters.</code></pre>
<p>The marginal energy plot is bimodal, which is quite unusual, and the <span class="math inline">\(\hat{R}\)</span> statistics are still quite high.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>make_diagnostic_plots(rot_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_79_0.png" title="fig:" alt="png" />
</center>
<p>To diagnose these issues, we investigate the posterior distributions of these noise parameters.</p>
<p>The posterior distribution of <span class="math inline">\(\sigma\)</span> appears to be bimodal, with one local maximum twice as high as the other. This would seem to indicate that two of our three chains spent their time in the mode corresponding two smaller values of <span class="math inline">\(\sigma\)</span> and the other chain spent its time in the mode corresponding to the higher value of <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(rot_trace, var_names<span class="op">=</span><span class="st">&quot;σ&quot;</span>, ref_val<span class="op">=</span>S_SCALE)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_81_0.png" title="fig:" alt="png" />
</center>
<p>Sure enough, plotting the trace of each chain confirms this suspiscion.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>az.plot_trace(rot_trace, var_names<span class="op">=</span><span class="st">&quot;σ&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_83_0.png" title="fig:" alt="png" />
</center>
<p>The pair plots for the unconstrained entries of <code>w_block</code> show that we have successfully broken the rotational invariance of the naive factor model, but also show bimodal behavior.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> az.plot_pair(rot_trace, var_names<span class="op">=</span><span class="st">&quot;w_block&quot;</span>,</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                   scatter_kwargs<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.5</span>})</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes.flat:</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="va">None</span>)<span class="op">;</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="va">None</span>)<span class="op">;</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].figure.tight_layout()<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_85_0.png" title="fig:" alt="png" />
</center>
<p>We see here that even though we appear to have broken the rotation invariance of the likelihood, it remains unidentified in the case that the signs of the <span class="math inline">\(\mathbf{w}_j\)</span>s and <span class="math inline">\(\mathbf{z}_i\)</span>s are changed in a consistent manner. To see this invariance, notice that if <span class="math inline">\(R \in \mathbb{R}^{2 \times 2}\)</span> is diagonal with nonzero entries in the set <span class="math inline">\(\{-1, 1\}\)</span>, then <span class="math inline">\(R^{\top} R = I_2\)</span> and the previous algebra shows that the likelihood is invariant under transformation of the latent parameters by <span class="math inline">\(R\)</span>.</p>
<h4 id="breaking-reflection-invariance">Breaking reflection invariance</h4>
<p>To break this reflection invariance, we fix the signs on one of the unconstrained factor loadings, <span class="math inline">\(\mathbf{w}_j\)</span> and preserve the relationship between its signs and the signs of the corresponding entries of the other <span class="math inline">\(\mathbf{w}_j\)</span>s and <span class="math inline">\(\mathbf{z}_i\)</span>s. We choose to fix the sign of the factor loading that has the largest <span class="math inline">\(\hat{R}\)</span> statistic in one of its columns, as this will be the loading with the most extreme reflection symmetry. We denote this loading’s index by <span class="math inline">\(\hat{j}\)</span>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>j_hat, <span class="op">=</span> (az.rhat(rot_trace, var_names<span class="op">=</span><span class="st">&quot;w_block&quot;</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>            .<span class="bu">max</span>(dim<span class="op">=</span><span class="st">&quot;w_block_dim_1&quot;</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>            .argmax(dim<span class="op">=</span><span class="st">&quot;w_block_dim_0&quot;</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>            .to_array()</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>            .data)</span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> az.plot_pair(rot_trace, var_names<span class="op">=</span><span class="st">&quot;w_block&quot;</span>,</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>                  coords<span class="op">=</span>{<span class="st">&quot;w_block_dim_0&quot;</span>: j_hat},</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>                  scatter_kwargs<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.25</span>})</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;&quot;</span>)<span class="op">;</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&quot;&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_89_0.png" title="fig:" alt="png" />
</center>
<p>We choose the target signs of the entries in <span class="math inline">\(\mathbf{w}_{\hat{j}}\)</span>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>target_sign <span class="op">=</span> np.sign(</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    rot_trace[<span class="st">&quot;posterior&quot;</span>][<span class="st">&quot;w_block&quot;</span>]</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>             [<span class="dv">0</span>, :, j_hat]</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>             .mean(dim<span class="op">=</span><span class="st">&quot;draw&quot;</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>             .data</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The priors on <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma\)</span>, are the same as for the previous model, as is most of the construction of <span class="math inline">\(\mathbf{w}_j\)</span> and <span class="math inline">\(\mathbf{z}_i\)</span>.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> ref_model:</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    β<span class="dv">0</span> <span class="op">=</span> pm.Normal(<span class="st">&quot;β0&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>M)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    β <span class="op">=</span> pm.Normal(<span class="st">&quot;β&quot;</span>, <span class="fl">0.</span>, <span class="fl">5.</span>, shape<span class="op">=</span>(M, K))</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> β<span class="dv">0</span> <span class="op">+</span> at.dot(X, β.T)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    σ <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;σ&quot;</span>, <span class="fl">2.5</span>)</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>w_top <span class="op">=</span> at.eye(M, F)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ref_model:</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    w_top_pos <span class="op">=</span> at.set_subtensor(</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        w_top[F],</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        pm.HalfNormal(<span class="st">&quot;w_pos&quot;</span>, HALFNORMAL_SCALE, shape<span class="op">=</span>F)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    w_block_ <span class="op">=</span> pm.Normal(<span class="st">&quot;w_block_&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(M <span class="op">-</span> (F <span class="op">+</span> <span class="dv">1</span>), F))</span></code></pre></div>
<p>We now enforce our choice of signs on <span class="math inline">\(\mathbf{w}_j\)</span> and <span class="math inline">\(\mathbf{z}_i\)</span>.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ref_model:</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    w_block <span class="op">=</span> pm.Deterministic(</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;w_block&quot;</span>,</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        target_sign <span class="op">*</span> at.sgn(w_block_[j_hat]) <span class="op">*</span> w_block_</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> pm.Deterministic(<span class="st">&quot;w&quot;</span>, at.set_subtensor(</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        w_top_pos[F <span class="op">+</span> <span class="dv">1</span>:], w_block    </span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    ))</span></code></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ref_model:</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    z_ <span class="op">=</span> pm.Normal(<span class="st">&quot;z_&quot;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>(N, F))</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> pm.Deterministic(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;z&quot;</span>, target_sign <span class="op">*</span> at.sgn(w_block_[j_hat]) <span class="op">*</span> z_</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>The likelihood of the observed data is the same as in the previous model.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ref_model:</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    obs <span class="op">=</span> pm.Normal(<span class="st">&quot;obs&quot;</span>, μ <span class="op">+</span> z.dot(w.T), σ, observed<span class="op">=</span>Y)</span></code></pre></div>
<p>Again we sample from the posterior distribution of this model.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> ref_model:</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    ref_trace <span class="op">=</span> pm.sample(<span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [β0, β, σ, w_pos, w_block_, z_]</code></pre>
<div>
<style>
/* Turns off some styling */
progress {
/* gets rid of default border in Firefox and Opera. */
border: none;
/* Needs to be in here for Safari polyfill so background images work as expected. */
background-size: auto;
}
.progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
background: #F44336;
}
</style>
<progress value="8000" class max="8000" style="width:300px; height:20px; vertical-align: middle;">
</progress>
<p>100.00% [8000/8000 04:37&lt;00:00 Sampling 4 chains, 16 divergences]</p>
</div>
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 278 seconds.
There were 16 divergences after tuning. Increase `target_accept` or reparameterize.
The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.
The estimated number of effective samples is smaller than 200 for some parameters.</code></pre>
<p>The marginal energy plot plot looks much better for this model, as do the <span class="math inline">\(\hat{R}\)</span> statistics. Note that here we have ignored the warnings about (and not plotted) the <span class="math inline">\(\hat{R}\)</span> statistics for the untransformed <code>w_block_</code> and <code>z_</code>, which we expect to be large, as they still are invariant under reflections.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>rhat_var_names <span class="op">=</span> [</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    var_name <span class="cf">for</span> var_name <span class="kw">in</span> ref_trace[<span class="st">&quot;posterior&quot;</span>].data_vars</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> var_name.endswith(<span class="st">&quot;_&quot;</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>make_diagnostic_plots(ref_trace, var_names<span class="op">=</span>rhat_var_names)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_103_0.png" title="fig:" alt="png" />
</center>
<p>Plotting the posterior distribution of <span class="math inline">\(w_{4, 0}\)</span>, we can see that we have in fact broken reflection invariance and fully identified our model.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> az.plot_pair(ref_trace, var_names<span class="op">=</span><span class="st">&quot;w_block&quot;</span>,</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>                  coords<span class="op">=</span>{<span class="st">&quot;w_block_dim_0&quot;</span>: j_hat},</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>                  scatter_kwargs<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.25</span>})</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;&quot;</span>)<span class="op">;</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&quot;&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_105_0.png" title="fig:" alt="png" />
</center>
<p>In this fully identified model, the posterior expected value of the parameters shows good agreements with the true values used to generate the data.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>ax, <span class="op">=</span> az.plot_forest(ref_trace, var_names<span class="op">=</span><span class="st">&quot;β0&quot;</span>, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>ax.scatter([], [],</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>           facecolors<span class="op">=</span><span class="st">&#39;none&#39;</span>, edgecolors<span class="op">=</span><span class="st">&#39;C0&#39;</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span>)<span class="op">;</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(B0, ax.get_yticks()[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>           c<span class="op">=</span><span class="st">&#39;k&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Actual&quot;</span>)<span class="op">;</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])<span class="op">;</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\beta_0$&quot;</span>)<span class="op">;</span></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">&quot;upper left&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_107_0.png" title="fig:" alt="png" />
</center>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>ax, <span class="op">=</span> az.plot_forest(ref_trace, var_names<span class="op">=</span><span class="st">&quot;β&quot;</span>,</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>                     coords<span class="op">=</span>{<span class="st">&quot;β_dim_1&quot;</span>: <span class="dv">0</span>},</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>                     hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>ax.scatter([], [],</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>           facecolors<span class="op">=</span><span class="st">&#39;none&#39;</span>, edgecolors<span class="op">=</span><span class="st">&#39;C0&#39;</span>,</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span>)<span class="op">;</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>ax.scatter(B[:, <span class="dv">0</span>], ax.get_yticks()[::<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>           c<span class="op">=</span><span class="st">&#39;k&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">&quot;Actual&quot;</span>)<span class="op">;</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])<span class="op">;</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\beta_{i, 0}$&quot;</span>)<span class="op">;</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">&quot;upper left&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_108_0.png" title="fig:" alt="png" />
</center>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(ref_trace, var_names<span class="op">=</span><span class="st">&quot;σ&quot;</span>, ref_val<span class="op">=</span>S_SCALE)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/factor_regression_files/factor_regression_109_0.png" title="fig:" alt="png" />
</center>
<p>Note that with more latent dimensions we would need to constrain the signs of more factor loadings to completely break reflection invariance.</p>
<p>This post is available as a Jupyter notebook <a href="https://nbviewer.jupyter.org/gist/AustinRochford/8691a869e85dceaf2b8333f710414936">here</a>.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv</span></code></pre></div>
<pre><code>Last updated: Mon Jul 05 2021

Python implementation: CPython
Python version       : 3.8.8
IPython version      : 7.22.0

arviz     : 0.11.2
pymc3     : 4.0
numpy     : 1.20.2
aesara    : 2.0.12
matplotlib: 3.4.1
seaborn   : 0.11.1</code></pre>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Peeters, Carel FW. “Rotational uniqueness conditions under oblique factor correlation metric.” <em>Psychometrika</em>, 77.2 (2012): 288-292.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
