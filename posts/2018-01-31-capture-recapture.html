<meta name="title" content="Capture-Recapture Models in Python with PyMC3" />
<meta name="tags" content="Bayesian Statistics, PyMC3, Ecology" />
<meta name="date" content="2018-01-31" />
<meta name="has_math" content="true" /><p>Recently, a project at work has led me to learn a bit about <a href="https://en.wikipedia.org/wiki/Mark_and_recapture">capture-recapture</a> models for estimating the size and dynamics of populations that cannot be fully enumerated. These models often arise in ecological statistics when estimating the size, birth, and survival rates of animal populations in the wild. This post gives examples of implementing three capture-recapture models in Python with <a href="https://github.com/pymc-devs/pymc3">PyMC3</a> and is intended primarily as a reference for my future self, though I hope it may serve as a useful introduction for others as well.</p>
<p>We will implement three Bayesian capture-recapture models:</p>
<ul>
<li>the <a href="https://en.wikipedia.org/wiki/Mark_and_recapture#Lincoln%E2%80%93Petersen_estimator">Lincoln-Petersen</a> model of abundance,</li>
<li>the Cormack-Jolly-Seber model of survival, and</li>
<li>the Jolly-Seber model of abundance and survival.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.ticker <span class="im">import</span> StrMethodFormatter</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc3 <span class="im">as</span> pm</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pymc3.distributions.dist_math <span class="im">import</span> binomln, bound, factln</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> theano</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> theano <span class="im">import</span> tensor <span class="im">as</span> tt</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>PCT_FORMATTER <span class="op">=</span> StrMethodFormatter(<span class="st">&#39;</span><span class="sc">{x:.1%}</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">518302</span> <span class="co"># from random.org, for reproducibility</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(SEED)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># keep theano from complaining about compile locks</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>(logging.getLogger(<span class="st">&#39;theano.gof.compilelock&#39;</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        .setLevel(logging.CRITICAL))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># keep theano from warning about default rounding mode changes</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>theano.config.warn.<span class="bu">round</span> <span class="op">=</span> <span class="va">False</span></span></code></pre></div>
<h2 id="the-lincoln-petersen-model">The Lincoln-Petersen model</h2>
<p>The simplest model of abundace, that is, the size of a population, is the Lincoln-Petersen model. While this model is a bit simple for most practical applications, it will introduce some useful modeling concepts and computational techniques.</p>
<p>The idea of the Lincoln-Petersen model is to visit the observation site twice to capture individuals from the population of interest. The individuals captured during the first visit are marked (often with tags, radio collars, microchips, etc.) and then released. The number of individuals captured, marked, and released is recorded as <span class="math inline">\(n_1\)</span>. On the second visit, the number of captured individuals is recorded as <span class="math inline">\(n_2\)</span>. If enough individuals are captured on the second visit, chances are quite high that several of them will have been marked on the first visit. The number of marked individuals recaptured on the second visit is recorded as <span class="math inline">\(n_{1, 2}\)</span>.</p>
<p>The Lincoln-Petersen model assumes that: 1. each individuals has an equal probability to be captured on both visits (regardless of whether or not they were marked), 2. no marks fall off or become illegible, and 3. the population is closed, that is, no individuals are born, die, enter, or leave the site between visits.</p>
<p>The third assumption is quite restrictive, and will be relaxed in the two subsequent models. The first two assumptions can be relaxed in various ways, but we will not do so in this post. First we derive a simple analytic estimator for the total population size given <span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span>, and <span class="math inline">\(n_{1, 2}\)</span>, then we fit a Bayesian Lincoln-Petersen model using PyMC3 to set the stage for the (Cormack-)Jolly-Seber models.</p>
<p>Let <span class="math inline">\(N\)</span> denote the size of the unknown total population, and let <span class="math inline">\(p\)</span> denote the capture probability. We have that</p>
<p><span class="math display">\[
\begin{align*}
n_1, n_2\ |\ N, p
    &amp; \sim \textrm{Bin}(N, p) \\
n_{1, 2}\ |\ n_1, p
    &amp; \sim \textrm{Bin}(n_1, p).
\end{align*}
\]</span></p>
<p>Therefore <span class="math inline">\(\frac{n_2}{N}\)</span> and <span class="math inline">\(\frac{n_{1, 2}}{n_1}\)</span> are unbiased estimates of <span class="math inline">\(p\)</span>. The Lincoln-Peterson estimator is derived by equating these estimators</p>
<p><span class="math display">\[\frac{n_2}{\hat{N}} = \frac{n_{1, 2}}{n_1}\]</span></p>
<p>and solving for</p>
<p><span class="math display">\[\hat{N} = \frac{n_1 n_2}{n_{1, 2}}.\]</span></p>
<p>We now simulate a data set where <span class="math inline">\(N = 1000\)</span> and the capture probability is <span class="math inline">\(p = 0.1\)</span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>N_LP <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>P_LP <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>x_lp <span class="op">=</span> sp.stats.bernoulli.rvs(P_LP, size<span class="op">=</span>(<span class="dv">2</span>, N_LP))</span></code></pre></div>
<p>The rows of <code>x_lp</code> correspond to site visits and the columns to individuals. The entry <code>x_lp[i, j]</code> is one if the <span class="math inline">\(j\)</span>-th individuals was captured on the <span class="math inline">\(i\)</span>-th site visit, and zero otherwise.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x_lp</span></code></pre></div>
<pre><code>array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 1, 0]])</code></pre>
<p>We construct <span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span>, and <span class="math inline">\(n_{1, 2}\)</span> from <code>x_lp</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n1, n2 <span class="op">=</span> x_lp.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>n12 <span class="op">=</span> x_lp.prod(axis<span class="op">=</span><span class="dv">0</span>).<span class="bu">sum</span>()</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>n1, n2, n12</span></code></pre></div>
<pre><code>(109, 95, 10)</code></pre>
<p>The Lincoln-Petersen estimate of <span class="math inline">\(N\)</span> is therefore</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>N_lp <span class="op">=</span> n1 <span class="op">*</span> n2 <span class="op">/</span> n12</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>N_lp</span></code></pre></div>
<pre><code>1035.5</code></pre>
<p>We now give a Bayesian formulation of the Lincoln-Petersen model. We use the priors</p>
<p><span class="math display">\[
\begin{align*}
p
    &amp; \sim U(0, 1) \\
\pi(N)
    &amp; = 1 \textrm{ for } N \geq n_1 + n_2 - n_{1, 2}.
\end{align*}
\]</span></p>
<p>Note that the prior on <span class="math inline">\(N\)</span> is improper.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> lp_model:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> pm.Uniform(<span class="st">&#39;p&#39;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    N_ <span class="op">=</span> pm.Bound(pm.Flat, lower<span class="op">=</span>n1 <span class="op">+</span> n2 <span class="op">-</span> n12)(<span class="st">&#39;N&#39;</span>)</span></code></pre></div>
<p>We now implement the likelihoods of the data given above during the derivation of the Lincoln-Petersen estimator.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> lp_model:</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    n1_obs <span class="op">=</span> pm.Binomial(<span class="st">&#39;n1_obs&#39;</span>, N_, p, observed<span class="op">=</span>n1)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    n2_obs <span class="op">=</span> pm.Binomial(<span class="st">&#39;n2_obs&#39;</span>, N_, p, observed<span class="op">=</span>n2)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    n12_obs <span class="op">=</span> pm.Binomial(<span class="st">&#39;n12_obs&#39;</span>, n1, p, observed<span class="op">=</span>n12)</span></code></pre></div>
<p>Now that the model is fully specified, we sample from its posterior distribution.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>NJOBS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>SAMPLE_KWARGS <span class="op">=</span> {</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;draws&#39;</span>: <span class="dv">1000</span>,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;njobs&#39;</span>: NJOBS,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;random_seed&#39;</span>: [SEED <span class="op">+</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(NJOBS)],</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;nuts_kwargs&#39;</span>: {<span class="st">&#39;target_accept&#39;</span>: <span class="fl">0.9</span>}</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> lp_model:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    lp_trace <span class="op">=</span> pm.sample(<span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 3 jobs)
NUTS: [N_lowerbound__, p_interval__]
100%|██████████| 1500/1500 [00:06&lt;00:00, 230.30it/s]
The number of effective samples is smaller than 25% for some parameters.</code></pre>
<p>First we examine a few sampling diagnostics. The Bayesian fraction of missing information (BFMI) and energy plot give no cause for concern.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pm.bfmi(lp_trace)</span></code></pre></div>
<pre><code>1.0584389661341544</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>pm.energyplot(lp_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_26_0.png" title="fig:" alt="png" />
</center>
<p>The Gelman-Rubin statistics are close to one, indicating convergence.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span>(np.<span class="bu">max</span>(gr_stats) <span class="cf">for</span> gr_stats <span class="kw">in</span> pm.gelman_rubin(lp_trace).values())</span></code></pre></div>
<pre><code>1.0040982739011743</code></pre>
<p>Since there are no apparent sampling problems, we examine the estimate of the population size.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    lp_trace, varnames<span class="op">=</span>[<span class="st">&#39;N&#39;</span>],</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    ref_val<span class="op">=</span>N_LP,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    lw<span class="op">=</span><span class="fl">0.</span>, alpha<span class="op">=</span><span class="fl">0.75</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_30_0.png" title="fig:" alt="png" />
</center>
<p>The true population size is well within the 95% credible interval. The posterior estimate of <span class="math inline">\(N\)</span> could become more accurate by substituting a reasonable upper bound for the prior on <span class="math inline">\(N\)</span> for the uninformative prior we have used here out of convenience.</p>
<h2 id="the-cormack-jolly-seber-model">The Cormack-Jolly-Seber model</h2>
<p>The Cormack-Jolly-Seber model estimates the survival dynamics of individuals in the population by relaxing the third assumption of the Lincoln-Petersen model, that the population is closed. Note that here “survival” does not necessarily correspond to the individual’s death, as it includes individuals that leave the observation area during the study. Despite this subtlety, we will use the convenient terminology of “alive” and “dead” throughout.</p>
<p>For the Cormack-Jolly-Seber and Jolly-Seber models, we follow the notation of <a href="http://capturerecapture.co.uk/"><em>Analysis of Capture-Recapture Data</em></a> by McCrea and Morgan. Additionally, we will use a <a href="https://en.wikipedia.org/wiki/Cormorant">cormorant</a> data set from the book to illustrate these two models. This data set involves eleven site visits where individuals were given individualized identifying marks. The data are defined below in the form of an <span class="math inline">\(M\)</span>-array.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.array([<span class="dv">30</span>, <span class="dv">157</span>, <span class="dv">174</span>, <span class="dv">298</span>, <span class="dv">470</span>, <span class="dv">421</span>, <span class="dv">413</span>, <span class="dv">514</span>, <span class="dv">430</span>, <span class="dv">181</span>])</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.zeros((T, T <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">0</span>, <span class="dv">1</span>:] <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">1</span>, <span class="dv">2</span>:] <span class="op">=</span> [<span class="dv">42</span>, <span class="dv">12</span>, <span class="dv">16</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">2</span>, <span class="dv">3</span>:] <span class="op">=</span> [<span class="dv">85</span>, <span class="dv">22</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">3</span>, <span class="dv">4</span>:] <span class="op">=</span> [<span class="dv">139</span>, <span class="dv">39</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">0</span>]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">4</span>, <span class="dv">5</span>:] <span class="op">=</span> [<span class="dv">175</span>, <span class="dv">60</span>, <span class="dv">22</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>]</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">5</span>, <span class="dv">6</span>:] <span class="op">=</span> [<span class="dv">159</span>, <span class="dv">46</span>, <span class="dv">16</span>, <span class="dv">5</span>, <span class="dv">2</span>]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">6</span>, <span class="dv">7</span>:] <span class="op">=</span> [<span class="dv">191</span>, <span class="dv">39</span>, <span class="dv">4</span>, <span class="dv">8</span>]</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">7</span>, <span class="dv">8</span>:] <span class="op">=</span> [<span class="dv">188</span>, <span class="dv">19</span>, <span class="dv">23</span>]</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">8</span>, <span class="dv">9</span>:] <span class="op">=</span> [<span class="dv">101</span>, <span class="dv">55</span>]</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>M[<span class="dv">9</span>, <span class="dv">10</span>] <span class="op">=</span> <span class="dv">84</span></span></code></pre></div>
<p>Here <code>T</code> indicates the number of revisits to the site so there were <span class="math inline">\(T + 1 = 11\)</span> total visits. The entries of <code>R</code> indicate the number of animals captured and released on each visit.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>R</span></code></pre></div>
<pre><code>array([ 30, 157, 174, 298, 470, 421, 413, 514, 430, 181])</code></pre>
<p>The entry <code>M[i, j]</code> indicates how many individuals captured on visit <span class="math inline">\(i\)</span> were first recaptured on visit <span class="math inline">\(j\)</span>. The diagonal of <code>M</code> is entirely zero, since it is not possible to recapture an individual on the same visit as it was released.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>M[:<span class="dv">4</span>, :<span class="dv">4</span>]</span></code></pre></div>
<pre><code>array([[  0.,  10.,   4.,   2.],
       [  0.,   0.,  42.,  12.],
       [  0.,   0.,   0.,  85.],
       [  0.,   0.,   0.,   0.]])</code></pre>
<p>Of the thirty individuals marked and released on the first visit, ten were recaptured on the second visit, four were recapture on the third visit, and so on.</p>
<p>Capture-recapture data is often given in the form of encounter histories, for example</p>
<p><span class="math display">\[1001000100,\]</span></p>
<p>which indicates that the individual was first captured on the first visit and recaptured on the fourth and eigth visits. It is straightforward to convert a series of encounter histories to an <span class="math inline">\(M\)</span>-array. We will discuss encounter histories again at the end of this post.</p>
<p>The parameters of the Cormack-Jolly-Seber model are <span class="math inline">\(p\)</span>, the capture probability, and <span class="math inline">\(\phi_i\)</span>, the probability that an individual that was alive during the <span class="math inline">\(i\)</span>-th visit is still alive during the <span class="math inline">\((i + 1)\)</span>-th visit. The capture probability can vary over time in the Cormack-Jolly-Seber model, but we use a constant capture probability here for simplicity.</p>
<p>We again place a uniform prior on <span class="math inline">\(p\)</span>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> cjs_model:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> pm.Uniform(<span class="st">&#39;p&#39;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>)</span></code></pre></div>
<p>We also place a uniform prior on <span class="math inline">\(\phi_i\)</span>.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    ϕ <span class="op">=</span> pm.Uniform(<span class="st">&#39;ϕ&#39;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>T)</span></code></pre></div>
<p>If <span class="math inline">\(\nu_{i, j}\)</span> is the probability associated with <code>M[i, j]</code>, then</p>
<p><span class="math display">\[
\begin{align*}
\nu_{i, j}
    &amp; = P(\textrm{individual that was alive at visit } i \textrm{ is alive at visit } j) \\
    &amp; \times P(\textrm{individual was not captured on visits } i + 1, \ldots, j - 1) \\
    &amp; \times P(\textrm{individual is captured on visit } j).
\end{align*}
\]</span></p>
<p>From our parameter definitions,</p>
<p><span class="math display">\[P(\textrm{individual that was alive at visit } i \textrm{ is alive at visit } j) = \prod_{k = i}^{j - 1} \phi_k,\]</span></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fill_lower_diag_ones(x):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tt.triu(x) <span class="op">+</span> tt.tril(tt.ones_like(x), k<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    p_alive <span class="op">=</span> tt.triu(</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        tt.cumprod(</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>            fill_lower_diag_ones(np.ones_like(M[:, <span class="dv">1</span>:]) <span class="op">*</span> ϕ),</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p><span class="math display">\[P(\textrm{individual was not captured at visits } i + 1, \ldots, j - 1) = (1 - p)^{j - i - 1},\]</span></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> np.arange(T)[:, np.newaxis]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> np.arange(T <span class="op">+</span> <span class="dv">1</span>)[np.newaxis]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>not_cap_visits <span class="op">=</span> np.clip(j <span class="op">-</span> i <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, np.inf)[:, <span class="dv">1</span>:]</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    p_not_cap <span class="op">=</span> tt.triu(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        (<span class="dv">1</span> <span class="op">-</span> p)<span class="op">**</span>not_cap_visits</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>and</p>
<p><span class="math display">\[P(\textrm{individual is captured on visit } j) = p.\]</span></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    ν <span class="op">=</span> p_alive <span class="op">*</span> p_not_cap <span class="op">*</span> p</span></code></pre></div>
<p>The likelihood of the observed recaptures is then</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>triu_i, triu_j <span class="op">=</span> np.triu_indices_from(M[:, <span class="dv">1</span>:])</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    recap_obs <span class="op">=</span> pm.Binomial(</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;recap_obs&#39;</span>,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        M[:, <span class="dv">1</span>:][triu_i, triu_j],</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        ν[triu_i, triu_j],</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        observed<span class="op">=</span>M[:, <span class="dv">1</span>:][triu_i, triu_j]</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>Finally, some individual released on each occasion are not recaptured again,</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">-</span> M.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>array([  12.,   83.,   53.,   94.,  199.,  193.,  171.,  284.,  274.,   97.])</code></pre>
<p>The probability of this event is</p>
<p><span class="math display">\[\chi_i = P(\textrm{released on visit } i \textrm{ and not recaptured again}) = 1 - \sum_{j = i + 1}^T \nu_{i, j}.\]</span></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    χ <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> ν.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<p>The likelihood of the individual that were not recaptured again is</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    no_recap_obs <span class="op">=</span> pm.Binomial(</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;no_recap_obs&#39;</span>,</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>        R <span class="op">-</span> M.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>), χ,</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>        observed<span class="op">=</span>R <span class="op">-</span> M.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>Now that the model is fully specified, we sample from its posterior distribution.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> cjs_model:</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    cjs_trace <span class="op">=</span> pm.sample(<span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 3 jobs)
NUTS: [ϕ_interval__, p_interval__]
100%|██████████| 1500/1500 [00:05&lt;00:00, 272.61it/s]</code></pre>
<p>Again, the BFMI and energy plot are reasonable.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>pm.bfmi(cjs_trace)</span></code></pre></div>
<pre><code>0.97973424667749842</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>pm.energyplot(cjs_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_63_0.png" title="fig:" alt="png" />
</center>
<p>The Gelman-Rubin statistics also indicate convergence.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span>(np.<span class="bu">max</span>(gr_stats) <span class="cf">for</span> gr_stats <span class="kw">in</span> pm.gelman_rubin(cjs_trace).values())</span></code></pre></div>
<pre><code>1.0009026695355843</code></pre>
<p>McCrea and Morgan’s book includes a table of maximum likelihood estimates for <span class="math inline">\(\phi_i\)</span> and <span class="math inline">\(p\)</span>. We verify that our Bayesian estimates are close to these.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>ϕ_mle <span class="op">=</span> np.array([<span class="fl">0.8</span>, <span class="fl">0.56</span>, <span class="fl">0.83</span>, <span class="fl">0.86</span>, <span class="fl">0.73</span>, <span class="fl">0.69</span>, <span class="fl">0.81</span>, <span class="fl">0.64</span>, <span class="fl">0.46</span>, <span class="fl">0.99</span>])</span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>t_plot <span class="op">=</span> np.arange(T) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>low, high <span class="op">=</span> np.percentile(cjs_trace[<span class="st">&#39;ϕ&#39;</span>], [<span class="dv">5</span>, <span class="dv">95</span>], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>ax.fill_between(</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    t_plot, low, high,</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">&quot;90</span><span class="sc">% i</span><span class="st">nterval&quot;</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    t_plot, cjs_trace[<span class="st">&#39;ϕ&#39;</span>].mean(axis<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>    t_plot, ϕ_mle,</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="st">&#39;k&#39;</span>, label<span class="op">=</span><span class="st">&quot;Maximum likelihood estimate&quot;</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">1</span>, T)<span class="op">;</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;$t$&quot;</span>)<span class="op">;</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>ax.yaxis.set_major_formatter(PCT_FORMATTER)<span class="op">;</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\phi_t$&quot;</span>)<span class="op">;</span></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_68_0.png" title="fig:" alt="png" />
</center>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>p_mle <span class="op">=</span> <span class="fl">0.51</span></span></code></pre></div>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pm.plot_posterior(</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    cjs_trace, varnames<span class="op">=</span>[<span class="st">&#39;p&#39;</span>],</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    ref_val<span class="op">=</span>p_mle,</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    lw<span class="op">=</span><span class="fl">0.</span>, alpha<span class="op">=</span><span class="fl">0.75</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&quot;$p$&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_70_0.png" title="fig:" alt="png" />
</center>
<h2 id="the-jolly-seber-model">The Jolly-Seber Model</h2>
<p>The Jolly-Seber model is an extension of the Cormack-Jolly-Seber model (the fact that the extension is named after fewer people is a bit counterintuitive) that estimates abundance and birth dynamics, in addition to the survival dynamics estimated by the Cormack-Jolly-Seber model. As with the Cormack-Jolly-Seber model where “death” included leaving the site, “birth” includes not just the actual birth of new individuals, but individuals that arrive at the site during the study from elsewhere. Again, despite this subtlety, we will use the convenient terminology of “birth” and “born” throughout.</p>
<p>In order to estimate abundance and birth dynamics, the Jolly-Seber model adds likelihood terms for the first time an individual is captured to the recapture likelihood of the Cormack-Jolly-Seber model. We use the same uniform priors on <span class="math inline">\(p\)</span> and <span class="math inline">\(\phi_i\)</span> as in the Cormack-Jolly-Seber model.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> js_model:</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> pm.Uniform(<span class="st">&#39;p&#39;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    ϕ <span class="op">=</span> pm.Uniform(<span class="st">&#39;ϕ&#39;</span>, <span class="fl">0.</span>, <span class="fl">1.</span>, shape<span class="op">=</span>T)</span></code></pre></div>
<p>As with the Lincoln-Petersen model, the Jolly-Seber model estimates the size of the population, including all individuals ever alive during the study period, <span class="math inline">\(N\)</span>. We use the Schwarz-Arnason formulation of the Jolly-Seber model, where each individual has probability <span class="math inline">\(\beta_i\)</span> of being born into the population between visits <span class="math inline">\(i\)</span> and <span class="math inline">\(i + 1\)</span>. We place a <span class="math inline">\(\operatorname{Dirichlet}(1, \ldots, 1)\)</span> prior on these parameters.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    β <span class="op">=</span> pm.Dirichlet(<span class="st">&#39;β&#39;</span>, np.ones(T), shape<span class="op">=</span>T)</span></code></pre></div>
<p>Let <span class="math inline">\(\Psi_i\)</span> denote the probability that a given individual is alive on visit <span class="math inline">\(i\)</span> and has not yet been captured before visit <span class="math inline">\(i\)</span>. Then <span class="math inline">\(\Psi_1 = \beta_0\)</span>, since no individuals can have been captured before the first visit, and</p>
<p><span class="math display">\[
\begin{align*}
\Psi_{i + 1}
    &amp; = P(\textrm{the individual was alive and unmarked during visit } i \textrm{ and survived to visit } i + 1) \\
    &amp; + P(\textrm{the individual was born between visits } i \textrm{ and } i + 1) \\
    &amp; = \Psi_i (1 - p) \phi_i + \beta_i.
\end{align*}
\]</span></p>
<p>After writing out the first few terms, we see that this recursion has the closed-form solution</p>
<p><span class="math display">\[\Psi_{i + 1} = \sum_{k = 0}^i \left(\beta_k (1 - p)^{i - k} \prod_{\ell = 1}^{i - k} \phi_{\ell} \right).\]</span></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>never_cap_surv_ix <span class="op">=</span> sp.linalg.circulant(np.arange(T))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    p_never_cap_surv <span class="op">=</span> tt.concatenate((</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">1</span>], tt.cumprod((<span class="dv">1</span> <span class="op">-</span> p) <span class="op">*</span> ϕ)[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    Ψ <span class="op">=</span> tt.tril(</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        β <span class="op">*</span> p_never_cap_surv[never_cap_surv_ix]</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    ).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<p>The probability that an unmarked individual that is alive at visit <span class="math inline">\(i\)</span> is captured on visit <span class="math inline">\(i\)</span> is then <span class="math inline">\(\Psi_i p\)</span>. The probability that an individual is alive at the end of the study period and never captured is <span class="math display">\[1 - \sum_{i = 1}^T \Psi_i p.\]</span></p>
<p>Therefore, the likelihood of the observed first captures is a <span class="math inline">\((T + 1)\)</span>-dimensional multinomial, where the first <span class="math inline">\(T\)</span> probabilities are <span class="math inline">\(\Psi_1 p, \ldots, \Psi_T p\)</span>, and the corresponding first <span class="math inline">\(T\)</span> counts are the observed number of unmarked individuals captured at each visit, <span class="math inline">\(u_i\)</span>. The final probability is</p>
<p><span class="math display">\[1 - \sum_{i = 1}^T \Psi_i p\]</span></p>
<p>and corresponds to the unobserved number of individuals never captured. Since PyMC3 does not implement such an “incomplete multinomial” distribution, we give a minimal implementation here.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IncompleteMultinomial(pm.Discrete):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n, p, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co">        n is the total frequency</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co">        p is the vector of probabilities of the observed components</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(IncompleteMultinomial, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean <span class="op">=</span> n <span class="op">*</span> p.<span class="bu">sum</span>() <span class="op">*</span> p,</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mode <span class="op">=</span> tt.cast(tt.<span class="bu">round</span>(n <span class="op">*</span> p), <span class="st">&#39;int32&#39;</span>)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> logp(<span class="va">self</span>, x):</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a><span class="co">        x is the vector of frequences of all but the last components</span></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="va">self</span>.n</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.p</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>        x_last <span class="op">=</span> n <span class="op">-</span> x.<span class="bu">sum</span>()</span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> bound(</span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>            factln(n) <span class="op">+</span> tt.<span class="bu">sum</span>(x <span class="op">*</span> tt.log(p) <span class="op">-</span> factln(x)) <span class="op">\</span></span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>                <span class="op">+</span> x_last <span class="op">*</span> tt.log(<span class="dv">1</span> <span class="op">-</span> p.<span class="bu">sum</span>()) <span class="op">-</span> factln(x_last),</span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a>            tt.<span class="bu">all</span>(x <span class="op">&gt;=</span> <span class="dv">0</span>), tt.<span class="bu">all</span>(x <span class="op">&lt;=</span> n), tt.<span class="bu">sum</span>(x) <span class="op">&lt;=</span> n,</span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a>            n <span class="op">&gt;=</span> <span class="dv">0</span>)</span></code></pre></div>
<p>As in the Lincoln-Petersen model, we place an improper flat prior (with the appropriate lower bound) on <span class="math inline">\(N\)</span>.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.concatenate(([R[<span class="dv">0</span>]], R[<span class="dv">1</span>:] <span class="op">-</span> M[:, <span class="dv">1</span>:].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)[:<span class="op">-</span><span class="dv">1</span>]))</span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> pm.Bound(pm.Flat, lower<span class="op">=</span>u.<span class="bu">sum</span>())(<span class="st">&#39;N&#39;</span>)</span></code></pre></div>
<p>The likelihood of the observed first captures is therefore</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    unmarked_obs <span class="op">=</span> IncompleteMultinomial(</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;unmarked_obs&#39;</span>, N, Ψ <span class="op">*</span> p,</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        observed<span class="op">=</span>u</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>The recapture likelihood for the Jolly-Seber model is the same as for the Cormack-Jolly-Seber model.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    p_alive <span class="op">=</span> tt.triu(</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>        tt.cumprod(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>            fill_lower_diag_ones(np.ones_like(M[:, <span class="dv">1</span>:]) <span class="op">*</span> ϕ),</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    p_not_cap <span class="op">=</span> tt.triu(</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        (<span class="dv">1</span> <span class="op">-</span> p)<span class="op">**</span>not_cap_visits</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    ν <span class="op">=</span> p_alive <span class="op">*</span> p_not_cap <span class="op">*</span> p</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    recap_obs <span class="op">=</span> pm.Binomial(</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;recap_obs&#39;</span>,</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>        M[:, <span class="dv">1</span>:][triu_i, triu_j], ν[triu_i, triu_j],</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        observed<span class="op">=</span>M[:, <span class="dv">1</span>:][triu_i, triu_j]</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    χ <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> ν.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    no_recap_obs <span class="op">=</span> pm.Binomial(</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;no_recap_obs&#39;</span>,</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        R <span class="op">-</span> M.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>), χ,</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        observed<span class="op">=</span>R <span class="op">-</span> M.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p>Again we sample from the posterior distribution of this model.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> js_model:</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    js_trace <span class="op">=</span> pm.sample(<span class="op">**</span>SAMPLE_KWARGS)</span></code></pre></div>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 3 jobs)
NUTS: [N_lowerbound__, β_stickbreaking__, ϕ_interval__, p_interval__]
100%|██████████| 1500/1500 [00:29&lt;00:00, 50.24it/s]</code></pre>
<p>Again, the BFMI and energy plot are reasonable.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>pm.bfmi(js_trace)</span></code></pre></div>
<pre><code>0.93076073875335852</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>pm.energyplot(js_trace)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_93_0.png" title="fig:" alt="png" />
</center>
<p>The Gelman-Rubin statistics also indicate convergence.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span>(np.<span class="bu">max</span>(gr_stats) <span class="cf">for</span> gr_stats <span class="kw">in</span> pm.gelman_rubin(js_trace).values())</span></code></pre></div>
<pre><code>1.0051087141503718</code></pre>
<p>The posterior expected survival rates are, somewhat surprisingly, still quite similar to the maximum likelihood estimates under the Cormack-Jolly-Seber model.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>low, high <span class="op">=</span> np.percentile(js_trace[<span class="st">&#39;ϕ&#39;</span>], [<span class="dv">5</span>, <span class="dv">95</span>], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>ax.fill_between(</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    t_plot, low, high,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">&quot;90</span><span class="sc">% i</span><span class="st">nterval&quot;</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    t_plot, js_trace[<span class="st">&#39;ϕ&#39;</span>].mean(axis<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    t_plot, ϕ_mle,</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    zorder<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span><span class="st">&#39;k&#39;</span>, label<span class="op">=</span><span class="st">&quot;Maximum likelihood estimate (CJS)&quot;</span></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">1</span>, T)<span class="op">;</span></span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;$t$&quot;</span>)<span class="op">;</span></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>ax.yaxis.set_major_formatter(PCT_FORMATTER)<span class="op">;</span></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\phi_t$&quot;</span>)<span class="op">;</span></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">&quot;upper center&quot;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_97_0.png" title="fig:" alt="png" />
</center>
<p>The following plot shows the estimated birth dynamics.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>low, high <span class="op">=</span> np.percentile(js_trace[<span class="st">&#39;β&#39;</span>], [<span class="dv">5</span>, <span class="dv">95</span>], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>ax.fill_between(</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    t_plot <span class="op">-</span> <span class="dv">1</span>, low, high,</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">&quot;90</span><span class="sc">% i</span><span class="st">nterval&quot;</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    t_plot <span class="op">-</span> <span class="dv">1</span>, js_trace[<span class="st">&#39;β&#39;</span>].mean(axis<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">&quot;Posterior expected value&quot;</span></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, T <span class="op">-</span> <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&quot;$t$&quot;</span>)<span class="op">;</span></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>ax.yaxis.set_major_formatter(PCT_FORMATTER)<span class="op">;</span></span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\beta_t$&quot;</span>)<span class="op">;</span></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_99_0.png" title="fig:" alt="png" />
</center>
<p>The posterior expected population size is about 30% larger than the number of distinct individuals marked.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>js_trace[<span class="st">&#39;N&#39;</span>].mean() <span class="op">/</span> u.<span class="bu">sum</span>()</span></code></pre></div>
<pre><code>1.2951923918464066</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    js_trace, varnames<span class="op">=</span>[<span class="st">&#39;N&#39;</span>],</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    lw<span class="op">=</span><span class="fl">0.</span>, alpha<span class="op">=</span><span class="fl">0.75</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/capture_recapture_files/capture_recapture_102_0.png" title="fig:" alt="png" />
</center>
<p>Now that we have estimated these three models, we return briefly to the topic of <span class="math inline">\(M\)</span>-arrays versus encounter histories. While <span class="math inline">\(M\)</span>-arrays are a convienent summary of encounter histories, they do not lend themselves to common extensions of these models to include individual random effects, trap-dependent recapture, etc. as readily as encounter histories. Two possibile approaches to include such effects are:</p>
<ol type="1">
<li>Use likelihoods for the (Cormack-)Jolly-Seber models based on encounter histories, which are a bit more complex than those based on <span class="math inline">\(M\)</span>-arrays.</li>
<li>Individual <span class="math inline">\(M\)</span>-arrays: transform each individual’s account history into an <span class="math inline">\(M\)</span>-array an stack them into a three-dimensional array of <span class="math inline">\(M\)</span>-arrays.</li>
</ol>
<p>We may explore one (or both) of these approaches in a future post.</p>
<p>Thanks to <a href="http://heydenberk.com/">Eric Heydenberk</a> for his feedback on a early draft of this post.</p>
<p>This post is available as a Jupyter notebook <a href="http://nbviewer.jupyter.org/gist/AustinRochford/e67cb0c628b3692ecc669190fe86990c">here</a>.</p>
