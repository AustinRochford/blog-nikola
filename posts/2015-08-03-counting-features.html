<meta name="title" content="Saving Memory by Counting Combinations of Features" />
<meta name="tags" content="Statistics, Python" />
<meta name="date" content="2015-08-03" />
<meta name="has_math" content="true" /><style>
.dataframe * {border-color: #c0c0c0 !important;}
.dataframe th{background: #eee;}
.dataframe td{
    background: #fff;
    text-align: right; 
    min-width:5em;
}

/* Format summary rows */
.dataframe-summary-row tr:last-child,
.dataframe-summary-col td:last-child{
background: #eee;
    font-weight: 500;
}
</style>
<p>For all the hype about big data, much value resides in the world’s medium and small data. Especially when we consider the length of the feedback loop and total analyst time invested, insights from small and medium data are quite attractive and economical. Personally, I find analyzing data that fits into memory quite convenient, and therefore, when I am confronted with a data set that does not fit in memory as-is, I am willing to spend a bit of time to try to manipulate it to fit into memory.</p>
<p>The first technique I usually turn to is to only store distinct rows of a data set, along with the count of the number of times that row appears in the data set. This technique is fairly simple to implement, especially when the data set is generated by a SQL query. If the initial query that generates the data set is</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> u, v, w <span class="kw">FROM</span> t;</span></code></pre></div>
<p>we would modify it to become</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> u, v, w, <span class="fu">COUNT</span>(<span class="dv">1</span>) <span class="kw">FROM</span> t <span class="kw">GROUP</span> <span class="kw">BY</span> u, v, w;</span></code></pre></div>
<p>We now generate a sample data set with both discrete and continuous features.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> __future__ <span class="im">import</span> division</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> patsy <span class="im">import</span> dmatrices, dmatrix</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels <span class="im">import</span> api <span class="im">as</span> sm</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.base.model <span class="im">import</span> GenericLikelihoodModel</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1545721</span>) <span class="co"># from random.org</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100001</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>u_min, u_max <span class="op">=</span> <span class="dv">0</span>, <span class="dv">100</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>v_p <span class="op">=</span> <span class="fl">0.6</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n_ws <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>ws <span class="op">=</span> sp.stats.norm.rvs(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>n_ws)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>w_min, w_max <span class="op">=</span> ws.<span class="bu">min</span>(), ws.<span class="bu">max</span>()</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;u&#39;</span>: np.random.randint(u_min, u_max, size<span class="op">=</span>N),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;v&#39;</span>: sp.stats.bernoulli.rvs(v_p, size<span class="op">=</span>N),</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;w&#39;</span>: np.random.choice(ws, size<span class="op">=</span>N, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    })</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
97
</td>
<td>
0
</td>
<td>
0.537397
</td>
</tr>
<tr>
<th>
1
</th>
<td>
79
</td>
<td>
1
</td>
<td>
1.536383
</td>
</tr>
<tr>
<th>
2
</th>
<td>
44
</td>
<td>
1
</td>
<td>
1.074817
</td>
</tr>
<tr>
<th>
3
</th>
<td>
51
</td>
<td>
0
</td>
<td>
-0.491210
</td>
</tr>
<tr>
<th>
4
</th>
<td>
47
</td>
<td>
1
</td>
<td>
1.592646
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br></p>
<p>We see that this data frame has just over 100,000 rows, but only about 10,000 distinct rows.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df.shape[<span class="dv">0</span>]</span></code></pre></div>
<pre><code>100001</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df.drop_duplicates().shape[<span class="dv">0</span>]</span></code></pre></div>
<pre><code>9997</code></pre>
<p>We now use <code>pandas</code>’ <code>groupby</code> method to produce a data frame that contains the count of each unique combination of <code>x</code>, <code>y</code>, and <code>z</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>count_df <span class="op">=</span> df.groupby(<span class="bu">list</span>(df.columns)).size()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>count_df.name <span class="op">=</span> <span class="st">&#39;count&#39;</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>count_df <span class="op">=</span> count_df.reset_index()</span></code></pre></div>
<p>In order to make later examples interesting, we shuffle the rows of the reduced data frame, because <code>pandas</code> automatically sorts the values we grouped on in the reduced data frame.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>shuffled_ixs <span class="op">=</span> count_df.index.values</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(shuffled_ixs)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>count_df <span class="op">=</span> count_df.iloc[shuffled_ixs].copy().reset_index(drop<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>count_df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
<th>
count
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0.425597
</td>
<td>
14
</td>
</tr>
<tr>
<th>
1
</th>
<td>
48
</td>
<td>
1
</td>
<td>
-0.993981
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
35
</td>
<td>
0
</td>
<td>
0.358156
</td>
<td>
9
</td>
</tr>
<tr>
<th>
3
</th>
<td>
19
</td>
<td>
1
</td>
<td>
-0.760298
</td>
<td>
17
</td>
</tr>
<tr>
<th>
4
</th>
<td>
40
</td>
<td>
1
</td>
<td>
-0.688514
</td>
<td>
13
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<p>Again, we see that we are storing 90% fewer rows. Although this data set has been artificially generated, I have seen space savings of up to 98% when applying this technique to real-world data sets.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>count_df.shape[<span class="dv">0</span>] <span class="op">/</span> N</span></code></pre></div>
<pre><code>0.0999690003099969</code></pre>
<p>This space savings allows me to analyze data sets which initially appear too large to fit in memory. For example, the computer I am writing this on has 16 GB of RAM. At a 90% space savings, I can comfortably analyze a data set that might otherwise be 80 GB in memory while leaving a healthy amount of memory for other processes. To me, the convenience and tight feedback loop that come with fitting a data set entirely in memory are hard to overstate.</p>
<p>As nice as it is to fit a data set into memory, it’s not very useful unless we can still analyze it. The rest of this post will show how we can perform standard operations on these summary data sets.</p>
<p>For convenience, we will separate the feature columns from the count columns.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>summ_df <span class="op">=</span> count_df[[<span class="st">&#39;u&#39;</span>, <span class="st">&#39;v&#39;</span>, <span class="st">&#39;w&#39;</span>]]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> count_df[<span class="st">&#39;count&#39;</span>]</span></code></pre></div>
<h3 id="mean">Mean</h3>
<p>Suppose we have a group of numbers <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>. Let the unique values among these numbers be denoted <span class="math inline">\(z_1, z_2, \ldots, z_m\)</span> and let <span class="math inline">\(n_j\)</span> be the number of times <span class="math inline">\(z_j\)</span> apears in the original group. The mean of the <span class="math inline">\(x_i\)</span>s is therefore</p>
<p><span class="math display">\[
\begin{align*}
\bar{x}
    &amp; = \frac{1}{n} \sum_{i = 1}^n x_i
      = \frac{1}{n} \sum_{j = 1}^m n_j z_j,
\end{align*}
\]</span></p>
<p>since we may group identical <span class="math inline">\(x_i\)</span>s into a single summand. Since <span class="math inline">\(n = \sum_{j = 1}^m n_j\)</span>, we can calculate the mean using the following function.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean(df, count):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df.mul(count, axis<span class="op">=</span><span class="dv">0</span>).<span class="bu">sum</span>() <span class="op">/</span> count.<span class="bu">sum</span>()</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>mean(summ_df, n)</span></code></pre></div>
<pre><code>u    49.308067
v     0.598704
w     0.170815
dtype: float64</code></pre>
<p>We see that the means calculated by our function agree with the means of the original data frame.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>df.mean(axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>u    49.308067
v     0.598704
w     0.170815
dtype: float64</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>np.allclose(mean(summ_df, n), df.mean(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>True</code></pre>
<h3 id="variance">Variance</h3>
<p>We can calculate the variance as</p>
<p><span class="math display">\[
\begin{align*}
    \sigma_x^2
        &amp; = \frac{1}{n - 1} \sum_{i = 1}^n \left(x_i - \bar{x}\right)^2
          = \frac{1}{n - 1} \sum_{j = 1}^m n_j \left(z_j - \bar{x}\right)^2
\end{align*}
\]</span></p>
<p>using the same trick of combining identical terms in the original sum. Again, this calculation is easy to implement in Python.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> var(df, count):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> mean(df, count)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.power(df <span class="op">-</span> mu, <span class="dv">2</span>).mul(count, axis<span class="op">=</span><span class="dv">0</span>).<span class="bu">sum</span>() <span class="op">/</span> (count.<span class="bu">sum</span>() <span class="op">-</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>var(summ_df, n)</span></code></pre></div>
<pre><code>u    830.025064
v      0.240260
w      1.099191
dtype: float64</code></pre>
<p>We see that the variances calculated by our function agree with the variances of the original data frame.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>df.var()</span></code></pre></div>
<pre><code>u    830.025064
v      0.240260
w      1.099191
dtype: float64</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>np.allclose(var(summ_df, n), df.var(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre></div>
<pre><code>True</code></pre>
<h3 id="histogram">Histogram</h3>
<p>Histograms are fundamental tools for exploratory data analysis. Fortunately, <code>pyplot</code>’s <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist"><code>hist</code></a> function easily accommodates summarized data using the <code>weights</code> optional argument.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>fig, (full_ax, summ_ax) <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>nbins <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>blue, green <span class="op">=</span> sns.color_palette()[:<span class="dv">2</span>]</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>full_ax.hist(df.w, bins<span class="op">=</span>nbins, color<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>, lw<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>full_ax.set_xlabel(<span class="st">&#39;$w$&#39;</span>)<span class="op">;</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>full_ax.set_ylabel(<span class="st">&#39;Count&#39;</span>)<span class="op">;</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>full_ax.set_title(<span class="st">&#39;Full data frame&#39;</span>)<span class="op">;</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>summ_ax.hist(summ_df.w, bins<span class="op">=</span>nbins, weights<span class="op">=</span>n, color<span class="op">=</span>green, alpha<span class="op">=</span><span class="fl">0.5</span>, lw<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>summ_ax.set_xlabel(<span class="st">&#39;$w$&#39;</span>)<span class="op">;</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>summ_ax.set_title(<span class="st">&#39;Summarized data frame&#39;</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_36_0.png" title="fig:" alt="Histograms" />
</center>
<p>We see that the histograms for <span class="math inline">\(w\)</span> produced from the full and summarized data frames are identical.</p>
<h3 id="quantiles">Quantiles</h3>
<p>Calculating the mean and variance of our summarized data frames was not too difficult. Calculating quantiles from this data frame is slightly more involved, though still not terribly hard.</p>
<p>Our implementation will rely on sorting the data frame. Though this implementation is not optimal from a computation complexity point of view, it is in keeping with the spirit of <code>pandas</code>’ <a href="https://github.com/pydata/pandas/blob/654e7397280be9a681fafcf8f70cfe3e20a9ef47/pandas/core/algorithms.py#L367">implementation of quantiles</a>. I have given some thought on how to implement <a href="https://en.wikipedia.org/wiki/Median_of_medians">linear time selection</a> on the summarized data frame, but have not yet worked out the details.</p>
<p>Before writing a function to calculate quantiles of a data frame with several columns, we will walk through the simpler case of computing the quartiles of a single series.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> summ_df.u</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>u.head()</span></code></pre></div>
<pre><code>0     0
1    48
2    35
3    19
4    40
Name: u, dtype: int64</code></pre>
<p>First we <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.argsort.html"><code>argsort</code></a> the series.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>sorted_ilocs <span class="op">=</span> u.argsort()</span></code></pre></div>
<p>We see that <code>u.iloc[sorted_ilocs]</code> will now be in ascending order.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>sorted_u <span class="op">=</span> u.iloc[sorted_ilocs]</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>(sorted_u[:<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;=</span> sorted_u[<span class="dv">1</span>:]).<span class="bu">all</span>()</span></code></pre></div>
<pre><code>True</code></pre>
<p>More importantly, <code>counts.iloc[sorted_ilocs]</code> will have the count of the smallest element of <code>u</code> first, the count of the second smallest element second, etc.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>sorted_n <span class="op">=</span> n.iloc[sorted_ilocs]</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>sorted_cumsum <span class="op">=</span> sorted_n.cumsum()</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>cdf <span class="op">=</span> (sorted_cumsum <span class="op">/</span> n.<span class="bu">sum</span>()).values</span></code></pre></div>
<p>Now, the <span class="math inline">\(i\)</span>-th location of <code>sorted_cumsum</code> will contain the number of elements of <code>u</code> less than or equal to the <span class="math inline">\(i\)</span>-th smallest element, and therefore <code>cdf</code> is the empirical cumulative distribution function of <code>u</code>. The following plot shows that this interpretation is correct.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>blue, _, red <span class="op">=</span> sns.color_palette()[:<span class="dv">3</span>]</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>ax.plot(sorted_u, cdf, c<span class="op">=</span>blue, label<span class="op">=</span><span class="st">&#39;Empirical CDF&#39;</span>)<span class="op">;</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>plot_u <span class="op">=</span> np.arange(<span class="dv">100</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>ax.plot(plot_u, sp.stats.randint.cdf(plot_u, u_min, u_max), <span class="st">&#39;--&#39;</span>, c<span class="op">=</span>red, label<span class="op">=</span><span class="st">&#39;Population CDF&#39;</span>)<span class="op">;</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;$u$&#39;</span>)<span class="op">;</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_49_0.png" title="fig:" alt="Cumulative distribution function" />
</center>
<p>If, for example, we wish to find the median of <code>u</code>, we want to find the first location in <code>cdf</code> which is greater than or equal to 0.5.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>median_iloc_in_sorted <span class="op">=</span> (cdf <span class="op">&lt;</span> <span class="fl">0.5</span>).argmin()</span></code></pre></div>
<p>The index of the median in <code>u</code> is therefore <code>sorted_ilocs.iloc[median_iloc_in_sorted]</code>, so the median of <code>u</code> is</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>u.iloc[sorted_ilocs.iloc[median_iloc_in_sorted]]</span></code></pre></div>
<pre><code>49</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>df.u.quantile(<span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>49.0</code></pre>
<p>We can generalize this method to calculate multiple quantiles simultaneously as follows.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.array([<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>])</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>u.iloc[sorted_ilocs.iloc[np.less.outer(cdf, q).argmin(axis<span class="op">=</span><span class="dv">0</span>)]]</span></code></pre></div>
<pre><code>2299    24
9079    49
1211    74
Name: u, dtype: int64</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>df.u.quantile(q)</span></code></pre></div>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.25</span>    <span class="dv">24</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="fl">0.50</span>    <span class="dv">49</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="fl">0.75</span>    <span class="dv">74</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>dtype: float64</span></code></pre></div>
<p>The array <code>np.less.outer(cdf, q).argmin(axis=0)</code> contains three columns, each of which contains the result of comparing <code>cdf</code> to an element of <code>q</code>. The following function generalizes this approach from series to data frames.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantile(df, count, q<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> np.ravel(q)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    sorted_ilocs <span class="op">=</span> df.<span class="bu">apply</span>(pd.Series.argsort)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    sorted_counts <span class="op">=</span> sorted_ilocs.<span class="bu">apply</span>(<span class="kw">lambda</span> s: count.iloc[s].values)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    cdf <span class="op">=</span> sorted_counts.cumsum() <span class="op">/</span> sorted_counts.<span class="bu">sum</span>()</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    q_ilocs_in_sorted_ilocs <span class="op">=</span> pd.DataFrame(np.less.outer(cdf.values, q).argmin(axis<span class="op">=</span><span class="dv">0</span>).T,</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>                                           columns<span class="op">=</span>df.columns)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    q_ilocs <span class="op">=</span> sorted_ilocs.<span class="bu">apply</span>(<span class="kw">lambda</span> s: s[q_ilocs_in_sorted_ilocs[s.name]].reset_index(drop<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    q_df <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> s: s.iloc[q_ilocs[s.name]].reset_index(drop<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    q_df.index <span class="op">=</span> q</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_df</span></code></pre></div>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>quantile(summ_df, n, q<span class="op">=</span>q)</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0.25
</th>
<td>
24
</td>
<td>
0
</td>
<td>
-0.688514
</td>
</tr>
<tr>
<th>
0.50
</th>
<td>
49
</td>
<td>
1
</td>
<td>
0.040036
</td>
</tr>
<tr>
<th>
0.75
</th>
<td>
74
</td>
<td>
1
</td>
<td>
1.074817
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>df.quantile(q<span class="op">=</span>q)</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0.25
</th>
<td>
24
</td>
<td>
0
</td>
<td>
-0.688514
</td>
</tr>
<tr>
<th>
0.50
</th>
<td>
49
</td>
<td>
1
</td>
<td>
0.040036
</td>
</tr>
<tr>
<th>
0.75
</th>
<td>
74
</td>
<td>
1
</td>
<td>
1.074817
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>np.allclose(quantile(summ_df, n, q<span class="op">=</span>q), df.quantile(q<span class="op">=</span>q))</span></code></pre></div>
<pre><code>True</code></pre>
<h3 id="bootstrapping">Bootstrapping</h3>
<p>Another important operation is <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a>. We will see two ways to perfom bootstrapping on the summary data set.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>n_boot <span class="op">=</span> <span class="dv">10000</span></span></code></pre></div>
<p>Key to both approaches to the bootstrap is knowing the proprotion of the data set that each distinct combination of features comprised.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> n <span class="op">/</span> n.<span class="bu">sum</span>()</span></code></pre></div>
<p>The two approaches differ in what type of data frame they produce. The first we will discuss produces a non-summarized data frame with non-unique rows, while the second produces a summarized data frame. Each fo these approaches to bootstrapping is useful in different situations.</p>
<h4 id="non-summarized-bootstrap">Non-summarized bootstrap</h4>
<p>To produce a non-summarized data frame, we generate a list of locations in <code>feature_df</code> based on <code>weights</code> using <a href="http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html"><code>numpy.random.choice</code></a>.</p>
<pre class="pythin"><code>boot_ixs = np.random.choice(summ_df.shape[0], size=n_boot, replace=True,
                            p=weights)</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>boot_df <span class="op">=</span> summ_df.iloc[boot_ixs]</span></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>boot_df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1171
</th>
<td>
47
</td>
<td>
1
</td>
<td>
-1.392235
</td>
</tr>
<tr>
<th>
9681
</th>
<td>
3
</td>
<td>
1
</td>
<td>
0.018521
</td>
</tr>
<tr>
<th>
6664
</th>
<td>
13
</td>
<td>
1
</td>
<td>
1.941207
</td>
</tr>
<tr>
<th>
8343
</th>
<td>
13
</td>
<td>
0
</td>
<td>
0.655181
</td>
</tr>
<tr>
<th>
3595
</th>
<td>
95
</td>
<td>
1
</td>
<td>
0.972592
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<p>We can verify that our bootstrapped data frame has (approximately) the same distribution as the original data frame using <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">Q-Q plots</a>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>boot_qs <span class="op">=</span> boot_df[[<span class="st">&#39;u&#39;</span>, <span class="st">&#39;w&#39;</span>]].quantile(q<span class="op">=</span>ps)</span></code></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>qs <span class="op">=</span> df[[<span class="st">&#39;u&#39;</span>, <span class="st">&#39;w&#39;</span>]].quantile(q<span class="op">=</span>ps)</span></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>blue <span class="op">=</span> sns.color_palette()[<span class="dv">0</span>]</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>ax.plot((u_min, u_max), (u_min, u_max), <span class="st">&#39;--&#39;</span>, c<span class="op">=</span><span class="st">&#39;k&#39;</span>, lw<span class="op">=</span><span class="fl">0.75</span>,</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">&#39;Perfect agreement&#39;</span>)<span class="op">;</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(qs.u, boot_qs.u, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(u_min, u_max)<span class="op">;</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Original quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(u_min, u_max)<span class="op">;</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Resampled quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Q-Q plot for $u$&#39;</span>)<span class="op">;</span></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_78_0.png" title="fig:" alt="Q-Q plot for u" />
</center>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>blue <span class="op">=</span> sns.color_palette()[<span class="dv">0</span>]</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>ax.plot((w_min, w_max), (w_min, w_max), <span class="st">&#39;--&#39;</span>, c<span class="op">=</span><span class="st">&#39;k&#39;</span>, lw<span class="op">=</span><span class="fl">0.75</span>,</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">&#39;Perfect agreement&#39;</span>)<span class="op">;</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(qs.w, boot_qs.w, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(w_min, w_max)<span class="op">;</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Original quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(w_min, w_max)<span class="op">;</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Resampled quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Q-Q plot for $w$&#39;</span>)<span class="op">;</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_79_0.png" title="fig:" alt="Q-Q plot for w" />
</center>
<p>We see that both of the resampled distributions agree quite closely with the original distributions. We have only produced Q-Q plots for <span class="math inline">\(u\)</span> and <span class="math inline">\(w\)</span> because <span class="math inline">\(v\)</span> is binary-valued.</p>
<p>While at first non-summarized boostrap resampling may appear to counteract the benefits of summarizing the original data frame, it can be quite useful when training and evaluating <a href="https://en.wikipedia.org/wiki/Online_machine_learning">online learning</a> algorithms, where iterating through the locations of the bootstrapped data in the original summarized data frame is efficient.</p>
<h4 id="summarized-bootstrap">Summarized bootstrap</h4>
<p>To produce a summarized data frame, the counts of the resampled data frame are sampled from a <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a> with event probabilities given by <code>weights</code>.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>boot_counts <span class="op">=</span> pd.Series(np.random.multinomial(n_boot, weights), name<span class="op">=</span><span class="st">&#39;count&#39;</span>)</span></code></pre></div>
<p>Again, we compare the distribution of our bootstrapped data frame to that of the original with Q-Q plots. Here our summarized quantile function is quite useful.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>boot_count_qs <span class="op">=</span> quantile(summ_df, boot_counts, q<span class="op">=</span>ps)</span></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>ax.plot((u_min, u_max), (u_min, u_max), <span class="st">&#39;--&#39;</span>, c<span class="op">=</span><span class="st">&#39;k&#39;</span>, lw<span class="op">=</span><span class="fl">0.75</span>,</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">&#39;Perfect agreement&#39;</span>)<span class="op">;</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(qs.u, boot_count_qs.u, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(u_min, u_max)<span class="op">;</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Original quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(u_min, u_max)<span class="op">;</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Resampled quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Q-Q plot for $u$&#39;</span>)<span class="op">;</span></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_85_0.png" title="fig:" alt="Q-Q plot for u" />
</center>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>ax.plot((w_min, w_max), (w_min, w_max), <span class="st">&#39;--&#39;</span>, c<span class="op">=</span><span class="st">&#39;k&#39;</span>, lw<span class="op">=</span><span class="fl">0.75</span>,</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">&#39;Perfect agreement&#39;</span>)<span class="op">;</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(qs.w, boot_count_qs.w, c<span class="op">=</span>blue, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(w_min, w_max)<span class="op">;</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Original quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(w_min, w_max)<span class="op">;</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Resampled quantiles&#39;</span>)<span class="op">;</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Q-Q plot for $w$&#39;</span>)<span class="op">;</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span></code></pre></div>
<center>
<img src="/resources/counting-features/2015-08-03-counting-features_86_0.png" title="fig:" alt="Q-Q plot for w" />
</center>
<p>Again, we see that both of the resampled distributions agree quite closely with the original distributions.</p>
<h3 id="linear-regression">Linear Regression</h3>
<p>Linear regression is among the most frequently used types of statistical inference, and it plays nicely with summarized data. Typically, we have a response variable <span class="math inline">\(y\)</span> that we wish to model as a linear combination of <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(w\)</span> as</p>
<p><span class="math display">\[
\begin{align*}
    y_i = \beta_0 + \beta_1 u_i + \beta_2 v_i + \beta_3 w_i + \varepsilon,
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(\varepsilon \sim N(0, \sigma^2)\)</span> is noise. We generate such a data set below (with <span class="math inline">\(\sigma = 0.1\)</span>).</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">3.</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">4.</span>, <span class="fl">2.</span>])</span></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>noise_std <span class="op">=</span> <span class="fl">0.1</span></span></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dmatrix(<span class="st">&#39;u + v + w&#39;</span>, data<span class="op">=</span>df)</span></code></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.Series(np.dot(X, beta), name<span class="op">=</span><span class="st">&#39;y&#39;</span>) <span class="op">+</span> sp.stats.norm.rvs(scale<span class="op">=</span>noise_std, size<span class="op">=</span>N)</span></code></pre></div>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>y.head()</span></code></pre></div>
<pre><code>0    7.862559
1    3.830585
2   -0.388246
3    1.047091
4    0.992082
Name: y, dtype: float64</code></pre>
<p>Each element of the series <code>y</code> corresponds to one row in the uncompressed data frame <code>df</code>. The <a href="http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html"><code>OLS</code></a> class from <a href="http://statsmodels.sourceforge.net/devel/index.html"><code>statsmodels</code></a> comes quite close to recovering the true regression coefficients.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>full_ols <span class="op">=</span> sm.OLS(y, X).fit()</span></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>full_ols.params</span></code></pre></div>
<pre><code>const   -2.999658
x1       0.099986
x2      -3.998997
x3       2.000317
dtype: float64</code></pre>
<p>To show how we can perform linear regression on the summarized data frame, we recall the the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares estimator</a> minimizes the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation">residual sum of squares</a>. The residual sum of squares is given by</p>
<p><span class="math display">\[
\begin{align*}
RSS
    &amp; = \sum_{i = 1}^n \left(y_i - \mathbf{x}_i \mathbf{\beta}^{\intercal}\right)^2.
\end{align*}
\]</span></p>
<p>Here <span class="math inline">\(\mathbf{x}_i = [1\ u_i\ v_i\ w_i]\)</span> is the <span class="math inline">\(i\)</span>-th row of the original data frame (with a constant added for the intercept) and <span class="math inline">\(\mathbf{\beta} = [\beta_0\ \beta_1\ \beta_2\ \beta_3]\)</span> is the row vector of regression coefficients. It would be tempting to rewrite <span class="math inline">\(RSS\)</span> by grouping the terms based on the row their features map to in the compressed data frame, but this approach would lead to incorrect results. Due to the stochastic noise term <span class="math inline">\(\varepsilon_i\)</span>, identical values of <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(w\)</span> can (and will <a href="https://en.wikipedia.org/wiki/Almost_surely">almost certainly</a>) map to different values of <span class="math inline">\(y\)</span>. We can see this phenomenon by calculating the range of <span class="math inline">\(y\)</span> grouped on <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(w\)</span>.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>reg_df <span class="op">=</span> pd.concat((y, df), axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>reg_df.groupby((<span class="st">&#39;u&#39;</span>, <span class="st">&#39;v&#39;</span>, <span class="st">&#39;w&#39;</span>)).y.<span class="bu">apply</span>(np.ptp).describe()</span></code></pre></div>
<pre><code>count    9997.000000
mean        0.297891
std         0.091815
min         0.000000
25%         0.237491
50%         0.296838
75%         0.358015
max         0.703418
Name: y, dtype: float64</code></pre>
<p>If <span class="math inline">\(y\)</span> were uniquely determined by <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(w\)</span>, we would expect the mean and quartiles of these ranges to be zero, which they are not. Fortunately, we can account for is difficulty with a bit of care.</p>
<p>Let <span class="math inline">\(S_j = \{i\ |\ \mathbf{x}_i = \mathbf{z}_j\}\)</span>, the set of row indices in the original data frame that correspond to the <span class="math inline">\(j\)</span>-th row in the summary data frame. Define <span class="math inline">\(\bar{y}_{(j)} = \frac{1}{n_j} \sum_{i \in S_j} y_i\)</span>, which is the mean of the response variables that correspond to <span class="math inline">\(\mathbf{z}_j\)</span>. Intuitively, since <span class="math inline">\(\varepsilon_i\)</span> has mean zero, <span class="math inline">\(\bar{y}_{(j)}\)</span> is our best unbiased estimate of <span class="math inline">\(\mathbf{z}_j \mathbf{\beta}^{\intercal}\)</span>. We will now show that regressing <span class="math inline">\(\sqrt{n_j} \bar{y}_{(j)}\)</span> on <span class="math inline">\(\sqrt{n_j} \mathbf{z}_j\)</span> gives the same results as the full regression. We use the standard trick of adding and subtracting the mean and get</p>
<p><span class="math display">\[
\begin{align*}
RSS
    &amp; = \sum_{j = 1}^m \sum_{i \in S_j} \left(y_i - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2 \\
    &amp; = \sum_{j = 1}^m \sum_{i \in S_j} \left(\left(y_i - \bar{y}_{(j)}\right) + \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)\right)^2 \\
    &amp; = \sum_{j = 1}^m \sum_{i \in S_j} \left(\left(y_i - \bar{y}_{(j)}\right)^2 + 2 \left(y_i - \bar{y}_{(j)}\right) \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right) + \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2\right).
\end{align*}
\]</span></p>
<p>As is usual in these situations, the cross term vanishes, since</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i \in S_j} \left(y_i - \bar{y}_{(j)}\right) \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)
    &amp; = \sum_{i \in S_j} \left(y_i \bar{y}_{(j)} - y_i \mathbf{z}_j \mathbf{\beta}^{\intercal} - \bar{y}_{(j)}^2 + \bar{y}_{(j)} \mathbf{z}_j \mathbf{\beta}^{\intercal}\right) \\
    &amp; = \bar{y}_{(j)} \sum_{i \in S_j} y_i - \mathbf{z}_j \mathbf{\beta}^{\intercal} \sum_{i \in S_j} y_i - n_j \bar{y}_{(j)}^2 + n_j \bar{y}_{(j)} \mathbf{z}_j \mathbf{\beta}^{\intercal} \\
    &amp; = n_j \bar{y}_{(j)}^2 - n_j \bar{y}_{(j)} \mathbf{z}_j \mathbf{\beta}^{\intercal} - n_j \bar{y}_{(j)}^2 + n_j \bar{y}_{(j)} \mathbf{z}_j \mathbf{\beta}^{\intercal} \\
    &amp; = 0.
\end{align*}
\]</span></p>
<p>Therefore we may decompose the residual sum of squares as</p>
<p><span class="math display">\[
\begin{align*}
RSS
    &amp; = \sum_{j = 1}^m \sum_{i \in S_j} \left(y_i - \bar{y}_{(j)}\right)^2 + \sum_{j = 1}^m \sum_{i \in S_j} \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2 \\
    &amp; = \sum_{j = 1}^m \sum_{i \in S_j} \left(y_i - \bar{y}_{(j)}\right)^2 + \sum_{j = 1}^m n_j \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2.
\end{align*}
\]</span></p>
<p>The important property of this decomposition is that the first sum does not depend on <span class="math inline">\(\mathbf{\beta}\)</span>, so minimizing <span class="math inline">\(RSS\)</span> with respect to <span class="math inline">\(\mathbf{\beta}\)</span> is equivalent to minimizing the second sum. We see that this second sum can be written as</p>
<p><span class="math display">\[
\begin{align*}
\sum_{j = 1}^m n_j \left(\bar{y}_{(j)} - \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2
    &amp; = \sum_{j = 1}^m \left(\sqrt{n_j} \bar{y}_{(j)} - \sqrt{n_j} \mathbf{z}_j \mathbf{\beta}^{\intercal}\right)^2
\end{align*},
\]</span></p>
<p>which is exactly the residual sum of squares for regressing <span class="math inline">\(\sqrt{n_j} \bar{y}_{(j)}\)</span> on <span class="math inline">\(\sqrt{n_j} \mathbf{z}_j\)</span>.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>summ_reg_df <span class="op">=</span> reg_df.groupby((<span class="st">&#39;u&#39;</span>, <span class="st">&#39;v&#39;</span>, <span class="st">&#39;w&#39;</span>)).y.mean().reset_index().iloc[shuffled_ixs].reset_index(drop<span class="op">=</span><span class="va">True</span>).copy()</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>summ_reg_df[<span class="st">&#39;n&#39;</span>] <span class="op">=</span> n</span></code></pre></div>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>summ_reg_df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
<th>
y
</th>
<th>
n
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0.425597
</td>
<td>
-2.173984
</td>
<td>
14
</td>
</tr>
<tr>
<th>
1
</th>
<td>
48
</td>
<td>
1
</td>
<td>
-0.993981
</td>
<td>
-4.174895
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
35
</td>
<td>
0
</td>
<td>
0.358156
</td>
<td>
1.252848
</td>
<td>
9
</td>
</tr>
<tr>
<th>
3
</th>
<td>
19
</td>
<td>
1
</td>
<td>
-0.760298
</td>
<td>
-6.612355
</td>
<td>
17
</td>
</tr>
<tr>
<th>
4
</th>
<td>
40
</td>
<td>
1
</td>
<td>
-0.688514
</td>
<td>
-4.379063
</td>
<td>
13
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<p>The design matrices for this summarized model are easy to construct using <a href="https://patsy.readthedocs.org/en/latest/"><code>patsy</code></a>.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>y_summ, X_summ <span class="op">=</span> dmatrices(<span class="st">&quot;&quot;&quot;</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="st">                           I(np.sqrt(n) * y) ~</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="st">                           np.sqrt(n) + I(np.sqrt(n) * u) + I(np.sqrt(n) * v) + I(np.sqrt(n) * w) - 1</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="st">                           &quot;&quot;&quot;</span>,</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>                           data<span class="op">=</span>summ_reg_df)</span></code></pre></div>
<p>Note that we must remove <code>patsy</code>’s constant column for the intercept and replace it with <code>np.sqrt(n)</code>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>summ_ols <span class="op">=</span> sm.OLS(y_summ, X_summ).fit()</span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>summ_ols.params</span></code></pre></div>
<pre><code>array([-2.99965783,  0.09998571, -3.99899718,  2.00031673])</code></pre>
<p>We see that the summarized regression produces the same parameter estimates as the full regression.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>np.allclose(full_ols.params, summ_ols.params)</span></code></pre></div>
<pre><code>True</code></pre>
<h3 id="logistic-regression">Logistic Regression</h3>
<p>As a final example of adapting common methods to summarized data frames, we will show how to fit a logistic regression model on a summarized data set by maximum likelihood.</p>
<p>We will use the model</p>
<p><span class="math display">\[P(s = 1\ |\ w) = \frac{1}{1 + \exp(-\mathbf{x} \gamma^{\intercal})}\]</span>.</p>
<p>As above, <span class="math inline">\(\mathbf{x}_i = [1\ u_i\ v_i\ w_i]\)</span>. The true value of <span class="math inline">\(\gamma\)</span> is</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> np.array([<span class="fl">1.</span>, <span class="fl">0.01</span>, <span class="op">-</span><span class="fl">1.</span>, <span class="op">-</span><span class="fl">2.</span>])</span></code></pre></div>
<p>We now generate samples from this model.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dmatrix(<span class="st">&#39;u + v + w&#39;</span>, data<span class="op">=</span>df)</span></code></pre></div>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> pd.Series(sp.special.expit(np.dot(X, gamma)), name<span class="op">=</span><span class="st">&#39;p&#39;</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> pd.Series(sp.stats.bernoulli.rvs(p), name<span class="op">=</span><span class="st">&#39;s&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>logit_df <span class="op">=</span> pd.concat((s, p, df), axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>logit_df.head()</span></code></pre></div>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
s
</th>
<th>
p
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0.709963
</td>
<td>
97
</td>
<td>
0
</td>
<td>
0.537397
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0
</td>
<td>
0.092560
</td>
<td>
79
</td>
<td>
1
</td>
<td>
1.536383
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
0.153211
</td>
<td>
44
</td>
<td>
1
</td>
<td>
1.074817
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
0.923609
</td>
<td>
51
</td>
<td>
0
</td>
<td>
-0.491210
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0
</td>
<td>
0.062077
</td>
<td>
47
</td>
<td>
1
</td>
<td>
1.592646
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<p>We first fit the logistic regression model to the full data frame.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>full_logit <span class="op">=</span> sm.Logit(s, X).fit()</span></code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.414221
         Iterations 7</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>full_logit.params</span></code></pre></div>
<pre><code>const    0.965283
x1       0.009944
x2      -0.966797
x3      -1.990506
dtype: float64</code></pre>
<p>We see that the estimates are quite close to the true parameters.</p>
<p>The technique used to adapt maximum likelihood estimation of logistic regression to the summarized data frame is quite elegant. The likelihood for the full data set is given by the fact that (given <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(w\)</span>) <span class="math inline">\(s\)</span> is <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distributed</a> with</p>
<p><span class="math display">\[s_i\ |\ \mathbf{x}_i \sim \operatorname{Ber}\left(\frac{1}{1 + \exp(-\mathbf{x}_i \gamma^{\intercal})}\right).\]</span></p>
<p>To derive the likelihood for the summarized data set, we count the number of successes (where <span class="math inline">\(s = 1\)</span>) for each unique combination of features <span class="math inline">\(\mathbf{z}_j\)</span>, and denote this quantity <span class="math inline">\(k_j\)</span>.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>summ_logit_df <span class="op">=</span> logit_df.groupby((<span class="st">&#39;u&#39;</span>, <span class="st">&#39;v&#39;</span>, <span class="st">&#39;w&#39;</span>)).s.<span class="bu">sum</span>().reset_index().iloc[shuffled_ixs].reset_index(drop<span class="op">=</span><span class="va">True</span>).copy()</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>summ_logit_df <span class="op">=</span> summ_logit_df.rename(columns<span class="op">=</span>{<span class="st">&#39;s&#39;</span>: <span class="st">&#39;k&#39;</span>})</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>summ_logit_df[<span class="st">&#39;n&#39;</span>] <span class="op">=</span> n</span></code></pre></div>
<pre><code>summ_logit_df.head()</code></pre>
<center>
<div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
u
</th>
<th>
v
</th>
<th>
w
</th>
<th>
k
</th>
<th>
n
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0.425597
</td>
<td>
5
</td>
<td>
14
</td>
</tr>
<tr>
<th>
1
</th>
<td>
48
</td>
<td>
1
</td>
<td>
-0.993981
</td>
<td>
7
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
35
</td>
<td>
0
</td>
<td>
0.358156
</td>
<td>
8
</td>
<td>
9
</td>
</tr>
<tr>
<th>
3
</th>
<td>
19
</td>
<td>
1
</td>
<td>
-0.760298
</td>
<td>
12
</td>
<td>
17
</td>
</tr>
<tr>
<th>
4
</th>
<td>
40
</td>
<td>
1
</td>
<td>
-0.688514
</td>
<td>
9
</td>
<td>
13
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p><br /></p>
<p>Now, instead of each row representing a single Bernoulli trial (as in the full data frame), each row represents <span class="math inline">\(n_j\)</span> trials, so we have that <span class="math inline">\(k_j\)</span> is (conditionally) <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomially distributed</a> with</p>
<p><span class="math display">\[k_j\ |\ \mathbf{z}_j \sim \operatorname{Bin}\left(n_j, \frac{1}{1 + \exp(-\mathbf{z}_j \gamma^{\intercal})}\right).\]</span></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>summ_logit_X <span class="op">=</span> dmatrix(<span class="st">&#39;u + v + w&#39;</span>, data<span class="op">=</span>summ_logit_df)</span></code></pre></div>
<p>As I have shown in a <a href="http://austinrochford.com/posts/2015-03-03-mle-python-statsmodels.html">previous post</a>, we can use <code>statsmodels</code>’ <a href="http://statsmodels.sourceforge.net/devel/dev/generated/statsmodels.base.model.GenericLikelihoodModel.html"><code>GenericLikelihoodModel</code></a> class to fit custom probability models by maximum likelihood. The model is implemented as follows.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SummaryLogit(GenericLikelihoodModel):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, endog, exog, n, <span class="op">**</span>qwargs):</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="co">        endog is the number of successes</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="co">        exog are the features</span></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="co">        n are the number of trials</span></span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SummaryLogit, <span class="va">self</span>).<span class="fu">__init__</span>(endog, exog, <span class="op">**</span>qwargs)</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nloglikeobs(<span class="va">self</span>, gamma):</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma is the vector of regression coefficients</span></span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a><span class="co">        returns the negative log likelihood of each of the observations for</span></span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a><span class="co">        the coefficients in gamma</span></span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> sp.special.expit(np.dot(<span class="va">self</span>.exog, gamma))</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>sp.stats.binom.logpmf(<span class="va">self</span>.endog, <span class="va">self</span>.n, p)</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, start_params<span class="op">=</span><span class="va">None</span>, maxiter<span class="op">=</span><span class="dv">10000</span>, maxfun<span class="op">=</span><span class="dv">5000</span>, <span class="op">**</span>qwargs):</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># wraps the GenericLikelihoodModel&#39;s fit method to set default start parameters</span></span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> start_params <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>            start_params <span class="op">=</span> np.zeros(<span class="va">self</span>.exog.shape[<span class="dv">1</span>])</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>(SummaryLogit, <span class="va">self</span>).fit(start_params<span class="op">=</span>start_params,</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>                                             maxiter<span class="op">=</span>maxiter, maxfun<span class="op">=</span>maxfun, <span class="op">**</span>qwargs)</span></code></pre></div>
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>summ_logit <span class="op">=</span> SummaryLogit(summ_logit_df.k, summ_logit_X, summ_logit_df.n).fit()</span></code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 1.317583
         Iterations: 357
         Function evaluations: 599</code></pre>
<p>Again, we get reasonable estimates of the regression coefficients, which are close to those obtained from the full data set.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>summ_logit.params</span></code></pre></div>
<pre><code>array([ 0.96527992,  0.00994322, -0.96680904, -1.99051485])</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>np.allclose(summ_logit.params, full_logit.params, rtol<span class="op">=</span><span class="dv">10</span><span class="op">**-</span><span class="dv">4</span>)</span></code></pre></div>
<pre><code>True</code></pre>
<h3 id="conclusion">Conclusion</h3>
<p>Hopefully this introduction to the technique of summarizing data sets has proved useful and will allow you to explore medium data more easily in the future. We have only scratched the surface on the types of statistical techniques that can be adapted to work on summarized data sets, but with a bit of ingenuity, many of the ideas in this post can apply to other models.</p>
<p>This post is available as an <a href="http://ipython.org/">IPython</a> notebook <a href="http://nbviewer.ipython.org/gist/AustinRochford/53114977dc0697fc9985">here</a>.</p>
